{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2644604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "plt.rcParams['font.size'] = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630edb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('../data/PRSA_data_2010.1.1-2014.12.31_Beijing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d5bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SEQUENZE CONTINUE (≥ 100 valori consecutivi senza missing)\n",
      "\n",
      "==========================================================================================\n",
      "#     Inizio                 Fine                   Lunghezza    % Dataset   \n",
      "==========================================================================================\n",
      "1     2010-01-02 00:00:00    2010-01-23 16:00:00    521          1.19        %\n",
      "2     2010-01-26 12:00:00    2010-02-14 01:00:00    446          1.02        %\n",
      "3     2010-02-14 03:00:00    2010-03-20 04:00:00    818          1.87        %\n",
      "4     2010-03-23 00:00:00    2010-03-30 05:00:00    174          0.40        %\n",
      "5     2010-04-01 13:00:00    2010-05-17 16:00:00    1,108        2.53        %\n",
      "6     2010-05-17 19:00:00    2010-05-27 18:00:00    240          0.55        %\n",
      "7     2010-05-28 00:00:00    2010-06-01 15:00:00    112          0.26        %\n",
      "8     2010-06-09 17:00:00    2010-06-20 07:00:00    255          0.58        %\n",
      "9     2010-06-23 10:00:00    2010-08-13 19:00:00    1,234        2.82        %\n",
      "10    2010-08-16 16:00:00    2010-09-11 05:00:00    614          1.40        %\n",
      "11    2010-09-11 22:00:00    2010-09-19 18:00:00    189          0.43        %\n",
      "12    2010-09-30 21:00:00    2010-10-12 13:00:00    281          0.64        %\n",
      "13    2010-10-12 16:00:00    2010-11-01 08:00:00    473          1.08        %\n",
      "14    2010-11-03 15:00:00    2010-11-22 10:00:00    452          1.03        %\n",
      "15    2010-11-22 13:00:00    2010-12-31 23:00:00    947          2.16        %\n",
      "16    2011-01-02 00:00:00    2011-01-08 15:00:00    160          0.37        %\n",
      "17    2011-01-10 16:00:00    2011-03-06 19:00:00    1,324        3.02        %\n",
      "18    2011-03-09 01:00:00    2011-03-17 20:00:00    212          0.48        %\n",
      "19    2011-03-24 01:00:00    2011-03-30 19:00:00    163          0.37        %\n",
      "20    2011-04-29 08:00:00    2011-05-07 09:00:00    194          0.44        %\n",
      "21    2011-05-07 11:00:00    2011-05-25 18:00:00    440          1.00        %\n",
      "22    2011-05-27 16:00:00    2011-05-31 19:00:00    100          0.23        %\n",
      "23    2011-06-06 11:00:00    2011-07-31 20:00:00    1,330        3.03        %\n",
      "24    2011-08-11 16:00:00    2011-08-21 21:00:00    246          0.56        %\n",
      "25    2011-08-24 10:00:00    2011-08-29 14:00:00    125          0.29        %\n",
      "26    2011-08-29 16:00:00    2011-09-26 11:00:00    668          1.52        %\n",
      "27    2011-09-26 13:00:00    2011-10-03 12:00:00    168          0.38        %\n",
      "28    2011-10-07 16:00:00    2011-10-12 15:00:00    120          0.27        %\n",
      "29    2011-10-12 17:00:00    2011-10-24 18:00:00    290          0.66        %\n",
      "30    2011-10-28 01:00:00    2011-11-01 10:00:00    106          0.24        %\n",
      "31    2011-11-01 13:00:00    2011-11-16 10:00:00    358          0.82        %\n",
      "32    2011-11-16 13:00:00    2011-11-30 10:00:00    334          0.76        %\n",
      "33    2011-11-30 12:00:00    2011-12-21 11:00:00    504          1.15        %\n",
      "34    2011-12-21 13:00:00    2012-01-14 09:00:00    573          1.31        %\n",
      "35    2012-01-17 12:00:00    2012-02-29 09:00:00    1,030        2.35        %\n",
      "36    2012-03-07 15:00:00    2012-03-14 15:00:00    169          0.39        %\n",
      "37    2012-03-14 17:00:00    2012-04-27 14:00:00    1,054        2.41        %\n",
      "38    2012-04-27 16:00:00    2012-05-04 16:00:00    169          0.39        %\n",
      "39    2012-05-04 18:00:00    2012-05-19 19:00:00    362          0.83        %\n",
      "40    2012-05-28 22:00:00    2012-06-15 17:00:00    428          0.98        %\n",
      "41    2012-06-24 03:00:00    2012-06-29 00:00:00    118          0.27        %\n",
      "42    2012-07-04 18:00:00    2012-07-12 19:00:00    194          0.44        %\n",
      "43    2012-07-12 22:00:00    2012-07-17 14:00:00    113          0.26        %\n",
      "44    2012-07-18 02:00:00    2012-07-26 23:00:00    214          0.49        %\n",
      "45    2012-08-03 17:00:00    2012-08-11 16:00:00    192          0.44        %\n",
      "46    2012-08-21 13:00:00    2012-09-14 21:00:00    585          1.33        %\n",
      "47    2012-09-16 06:00:00    2012-09-26 03:00:00    238          0.54        %\n",
      "48    2012-09-27 13:00:00    2012-10-03 02:00:00    134          0.31        %\n",
      "49    2012-10-03 04:00:00    2012-10-16 11:00:00    320          0.73        %\n",
      "50    2012-10-20 17:00:00    2012-10-30 10:00:00    234          0.53        %\n",
      "51    2012-10-30 12:00:00    2012-11-19 19:00:00    488          1.11        %\n",
      "52    2012-11-20 16:00:00    2012-11-26 15:00:00    144          0.33        %\n",
      "53    2012-11-26 18:00:00    2012-12-09 05:00:00    300          0.68        %\n",
      "54    2012-12-09 07:00:00    2012-12-20 06:00:00    264          0.60        %\n",
      "55    2012-12-28 13:00:00    2013-01-12 08:00:00    356          0.81        %\n",
      "56    2013-01-16 03:00:00    2013-01-29 04:00:00    314          0.72        %\n",
      "57    2013-01-29 06:00:00    2013-02-14 10:00:00    389          0.89        %\n",
      "58    2013-02-14 12:00:00    2013-03-04 10:00:00    431          0.98        %\n",
      "59    2013-03-06 20:00:00    2013-03-14 11:00:00    184          0.42        %\n",
      "60    2013-03-16 08:00:00    2013-03-25 11:00:00    220          0.50        %\n",
      "61    2013-03-25 13:00:00    2013-04-02 07:00:00    187          0.43        %\n",
      "62    2013-04-02 09:00:00    2013-04-11 09:00:00    217          0.50        %\n",
      "63    2013-04-11 11:00:00    2013-04-15 16:00:00    102          0.23        %\n",
      "64    2013-04-16 18:00:00    2013-05-17 15:00:00    742          1.69        %\n",
      "65    2013-05-18 08:00:00    2013-05-28 12:00:00    245          0.56        %\n",
      "66    2013-05-28 14:00:00    2013-06-20 10:00:00    549          1.25        %\n",
      "67    2013-06-20 13:00:00    2013-07-17 13:00:00    649          1.48        %\n",
      "68    2013-07-17 15:00:00    2013-07-22 10:00:00    116          0.26        %\n",
      "69    2013-07-22 17:00:00    2013-08-18 23:00:00    655          1.49        %\n",
      "70    2013-08-22 16:00:00    2013-08-30 12:00:00    189          0.43        %\n",
      "71    2013-08-30 17:00:00    2013-09-13 08:00:00    328          0.75        %\n",
      "72    2013-09-13 11:00:00    2013-09-26 13:00:00    315          0.72        %\n",
      "73    2013-09-26 15:00:00    2013-10-08 13:00:00    287          0.65        %\n",
      "74    2013-10-08 18:00:00    2013-10-22 14:00:00    333          0.76        %\n",
      "75    2013-10-22 17:00:00    2013-11-19 15:00:00    671          1.53        %\n",
      "76    2013-11-23 03:00:00    2013-12-20 16:00:00    662          1.51        %\n",
      "77    2013-12-20 19:00:00    2013-12-28 19:00:00    193          0.44        %\n",
      "78    2013-12-30 11:00:00    2014-01-12 00:00:00    302          0.69        %\n",
      "79    2014-01-12 06:00:00    2014-01-22 15:00:00    250          0.57        %\n",
      "80    2014-01-22 18:00:00    2014-02-18 16:00:00    647          1.48        %\n",
      "81    2014-02-23 03:00:00    2014-03-21 15:00:00    637          1.45        %\n",
      "82    2014-03-21 17:00:00    2014-04-10 14:00:00    478          1.09        %\n",
      "83    2014-04-14 18:00:00    2014-05-20 10:00:00    857          1.96        %\n",
      "84    2014-05-20 13:00:00    2014-06-04 23:00:00    371          0.85        %\n",
      "85    2014-06-09 11:00:00    2014-06-18 13:00:00    219          0.50        %\n",
      "86    2014-06-18 18:00:00    2014-07-12 12:00:00    571          1.30        %\n",
      "87    2014-07-12 18:00:00    2014-07-22 10:00:00    233          0.53        %\n",
      "88    2014-07-22 13:00:00    2014-08-18 15:00:00    651          1.49        %\n",
      "89    2014-08-21 01:00:00    2014-09-16 06:00:00    630          1.44        %\n",
      "90    2014-09-23 20:00:00    2014-10-20 15:00:00    644          1.47        %\n",
      "91    2014-10-20 17:00:00    2014-11-06 09:00:00    401          0.92        %\n",
      "92    2014-11-06 11:00:00    2014-11-13 16:00:00    174          0.40        %\n",
      "93    2014-11-13 18:00:00    2014-11-20 20:00:00    171          0.39        %\n",
      "94    2014-11-21 09:00:00    2014-12-05 13:00:00    341          0.78        %\n",
      "95    2014-12-09 12:00:00    2014-12-20 07:00:00    260          0.59        %\n",
      "96    2014-12-20 17:00:00    2014-12-31 23:00:00    271          0.62        %\n",
      "==========================================================================================\n",
      "\n",
      "Statistiche:\n",
      "   Numero sequenze trovate:     96\n",
      "   Totale valori in sequenze:   38,471\n",
      "   % dataset utilizzabile:      87.79%\n",
      "   Media lunghezza sequenza:    401 valori\n",
      "   Sequenza più lunga:          1,330 valori\n",
      "   Sequenza più corta:          100 valori\n"
     ]
    }
   ],
   "source": [
    "# Funzione per identificare sequenze continue senza missing\n",
    "def find_continuous_sequences(series, min_length=100):\n",
    "    \"\"\"\n",
    "    Identifica sequenze continue di valori non-missing di lunghezza >= min_length.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Serie temporale da analizzare\n",
    "    min_length : int\n",
    "        Lunghezza minima della sequenza continua\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples : [(start_idx, end_idx, length), ...]\n",
    "        Lista di tuple con indici di inizio, fine e lunghezza di ogni sequenza\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    start_idx = None\n",
    "    current_length = 0\n",
    "    \n",
    "    for idx in range(len(series)):\n",
    "        if pd.notna(series.iloc[idx]):\n",
    "            # Valore valido\n",
    "            if start_idx is None:\n",
    "                start_idx = idx\n",
    "                current_length = 1\n",
    "            else:\n",
    "                current_length += 1\n",
    "        else:\n",
    "            # Missing value - salva sequenza se abbastanza lunga\n",
    "            if start_idx is not None and current_length >= min_length:\n",
    "                sequences.append((start_idx, idx - 1, current_length))\n",
    "            start_idx = None\n",
    "            current_length = 0\n",
    "    \n",
    "    # Controlla l'ultima sequenza\n",
    "    if start_idx is not None and current_length >= min_length:\n",
    "        sequences.append((start_idx, len(series) - 1, current_length))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Crea indice datetime\n",
    "df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Trova sequenze continue di PM2.5\n",
    "min_sequence_length = 100\n",
    "continuous_sequences = find_continuous_sequences(df['pm2.5'], min_length=min_sequence_length)\n",
    "\n",
    "print(f\"\\n SEQUENZE CONTINUE (≥ {min_sequence_length} valori consecutivi senza missing)\\n\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'#':<5} {'Inizio':<22} {'Fine':<22} {'Lunghezza':<12} {'% Dataset':<12}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "total_continuous_values = 0\n",
    "for i, (start_idx, end_idx, length) in enumerate(continuous_sequences, 1):\n",
    "    start_date = df.index[start_idx]\n",
    "    end_date = df.index[end_idx]\n",
    "    pct = length / len(df) * 100\n",
    "    print(f\"{i:<5} {str(start_date):<22} {str(end_date):<22} {length:<12,} {pct:<12.2f}%\")\n",
    "    total_continuous_values += length\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nStatistiche:\")\n",
    "print(f\"   Numero sequenze trovate:     {len(continuous_sequences)}\")\n",
    "print(f\"   Totale valori in sequenze:   {total_continuous_values:,}\")\n",
    "print(f\"   % dataset utilizzabile:      {total_continuous_values/len(df)*100:.2f}%\")\n",
    "print(f\"   Media lunghezza sequenza:    {total_continuous_values/len(continuous_sequences):.0f} valori\")\n",
    "print(f\"   Sequenza più lunga:          {max([s[2] for s in continuous_sequences]):,} valori\")\n",
    "print(f\"   Sequenza più corta:          {min([s[2] for s in continuous_sequences]):,} valori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096fe63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Continuo Estratto\n",
      "\n",
      "==========================================================================================\n",
      "Shape dataset originale:    (43824, 13)\n",
      "Shape dataset continuo:     (38471, 14)\n",
      "Righe mantenute:            38,471 (87.79%)\n",
      "==========================================================================================\n",
      "\n",
      "Informazioni Sequenze nel Dataset Continuo:\n",
      "==========================================================================================\n",
      "Seq   Inizio Originale     Fine Originale       Pos. Nuovo Dataset        Lunghezza   \n",
      "==========================================================================================\n",
      "1     2010-01-02 00:00:00  2010-01-23 16:00:00  0 - 520                   521         \n",
      "2     2010-01-26 12:00:00  2010-02-14 01:00:00  521 - 966                 446         \n",
      "3     2010-02-14 03:00:00  2010-03-20 04:00:00  967 - 1784                818         \n",
      "4     2010-03-23 00:00:00  2010-03-30 05:00:00  1785 - 1958               174         \n",
      "5     2010-04-01 13:00:00  2010-05-17 16:00:00  1959 - 3066               1,108       \n",
      "6     2010-05-17 19:00:00  2010-05-27 18:00:00  3067 - 3306               240         \n",
      "7     2010-05-28 00:00:00  2010-06-01 15:00:00  3307 - 3418               112         \n",
      "8     2010-06-09 17:00:00  2010-06-20 07:00:00  3419 - 3673               255         \n",
      "9     2010-06-23 10:00:00  2010-08-13 19:00:00  3674 - 4907               1,234       \n",
      "10    2010-08-16 16:00:00  2010-09-11 05:00:00  4908 - 5521               614         \n",
      "11    2010-09-11 22:00:00  2010-09-19 18:00:00  5522 - 5710               189         \n",
      "12    2010-09-30 21:00:00  2010-10-12 13:00:00  5711 - 5991               281         \n",
      "13    2010-10-12 16:00:00  2010-11-01 08:00:00  5992 - 6464               473         \n",
      "14    2010-11-03 15:00:00  2010-11-22 10:00:00  6465 - 6916               452         \n",
      "15    2010-11-22 13:00:00  2010-12-31 23:00:00  6917 - 7863               947         \n",
      "16    2011-01-02 00:00:00  2011-01-08 15:00:00  7864 - 8023               160         \n",
      "17    2011-01-10 16:00:00  2011-03-06 19:00:00  8024 - 9347               1,324       \n",
      "18    2011-03-09 01:00:00  2011-03-17 20:00:00  9348 - 9559               212         \n",
      "19    2011-03-24 01:00:00  2011-03-30 19:00:00  9560 - 9722               163         \n",
      "20    2011-04-29 08:00:00  2011-05-07 09:00:00  9723 - 9916               194         \n",
      "21    2011-05-07 11:00:00  2011-05-25 18:00:00  9917 - 10356              440         \n",
      "22    2011-05-27 16:00:00  2011-05-31 19:00:00  10357 - 10456             100         \n",
      "23    2011-06-06 11:00:00  2011-07-31 20:00:00  10457 - 11786             1,330       \n",
      "24    2011-08-11 16:00:00  2011-08-21 21:00:00  11787 - 12032             246         \n",
      "25    2011-08-24 10:00:00  2011-08-29 14:00:00  12033 - 12157             125         \n",
      "26    2011-08-29 16:00:00  2011-09-26 11:00:00  12158 - 12825             668         \n",
      "27    2011-09-26 13:00:00  2011-10-03 12:00:00  12826 - 12993             168         \n",
      "28    2011-10-07 16:00:00  2011-10-12 15:00:00  12994 - 13113             120         \n",
      "29    2011-10-12 17:00:00  2011-10-24 18:00:00  13114 - 13403             290         \n",
      "30    2011-10-28 01:00:00  2011-11-01 10:00:00  13404 - 13509             106         \n",
      "31    2011-11-01 13:00:00  2011-11-16 10:00:00  13510 - 13867             358         \n",
      "32    2011-11-16 13:00:00  2011-11-30 10:00:00  13868 - 14201             334         \n",
      "33    2011-11-30 12:00:00  2011-12-21 11:00:00  14202 - 14705             504         \n",
      "34    2011-12-21 13:00:00  2012-01-14 09:00:00  14706 - 15278             573         \n",
      "35    2012-01-17 12:00:00  2012-02-29 09:00:00  15279 - 16308             1,030       \n",
      "36    2012-03-07 15:00:00  2012-03-14 15:00:00  16309 - 16477             169         \n",
      "37    2012-03-14 17:00:00  2012-04-27 14:00:00  16478 - 17531             1,054       \n",
      "38    2012-04-27 16:00:00  2012-05-04 16:00:00  17532 - 17700             169         \n",
      "39    2012-05-04 18:00:00  2012-05-19 19:00:00  17701 - 18062             362         \n",
      "40    2012-05-28 22:00:00  2012-06-15 17:00:00  18063 - 18490             428         \n",
      "41    2012-06-24 03:00:00  2012-06-29 00:00:00  18491 - 18608             118         \n",
      "42    2012-07-04 18:00:00  2012-07-12 19:00:00  18609 - 18802             194         \n",
      "43    2012-07-12 22:00:00  2012-07-17 14:00:00  18803 - 18915             113         \n",
      "44    2012-07-18 02:00:00  2012-07-26 23:00:00  18916 - 19129             214         \n",
      "45    2012-08-03 17:00:00  2012-08-11 16:00:00  19130 - 19321             192         \n",
      "46    2012-08-21 13:00:00  2012-09-14 21:00:00  19322 - 19906             585         \n",
      "47    2012-09-16 06:00:00  2012-09-26 03:00:00  19907 - 20144             238         \n",
      "48    2012-09-27 13:00:00  2012-10-03 02:00:00  20145 - 20278             134         \n",
      "49    2012-10-03 04:00:00  2012-10-16 11:00:00  20279 - 20598             320         \n",
      "50    2012-10-20 17:00:00  2012-10-30 10:00:00  20599 - 20832             234         \n",
      "51    2012-10-30 12:00:00  2012-11-19 19:00:00  20833 - 21320             488         \n",
      "52    2012-11-20 16:00:00  2012-11-26 15:00:00  21321 - 21464             144         \n",
      "53    2012-11-26 18:00:00  2012-12-09 05:00:00  21465 - 21764             300         \n",
      "54    2012-12-09 07:00:00  2012-12-20 06:00:00  21765 - 22028             264         \n",
      "55    2012-12-28 13:00:00  2013-01-12 08:00:00  22029 - 22384             356         \n",
      "56    2013-01-16 03:00:00  2013-01-29 04:00:00  22385 - 22698             314         \n",
      "57    2013-01-29 06:00:00  2013-02-14 10:00:00  22699 - 23087             389         \n",
      "58    2013-02-14 12:00:00  2013-03-04 10:00:00  23088 - 23518             431         \n",
      "59    2013-03-06 20:00:00  2013-03-14 11:00:00  23519 - 23702             184         \n",
      "60    2013-03-16 08:00:00  2013-03-25 11:00:00  23703 - 23922             220         \n",
      "61    2013-03-25 13:00:00  2013-04-02 07:00:00  23923 - 24109             187         \n",
      "62    2013-04-02 09:00:00  2013-04-11 09:00:00  24110 - 24326             217         \n",
      "63    2013-04-11 11:00:00  2013-04-15 16:00:00  24327 - 24428             102         \n",
      "64    2013-04-16 18:00:00  2013-05-17 15:00:00  24429 - 25170             742         \n",
      "65    2013-05-18 08:00:00  2013-05-28 12:00:00  25171 - 25415             245         \n",
      "66    2013-05-28 14:00:00  2013-06-20 10:00:00  25416 - 25964             549         \n",
      "67    2013-06-20 13:00:00  2013-07-17 13:00:00  25965 - 26613             649         \n",
      "68    2013-07-17 15:00:00  2013-07-22 10:00:00  26614 - 26729             116         \n",
      "69    2013-07-22 17:00:00  2013-08-18 23:00:00  26730 - 27384             655         \n",
      "70    2013-08-22 16:00:00  2013-08-30 12:00:00  27385 - 27573             189         \n",
      "71    2013-08-30 17:00:00  2013-09-13 08:00:00  27574 - 27901             328         \n",
      "72    2013-09-13 11:00:00  2013-09-26 13:00:00  27902 - 28216             315         \n",
      "73    2013-09-26 15:00:00  2013-10-08 13:00:00  28217 - 28503             287         \n",
      "74    2013-10-08 18:00:00  2013-10-22 14:00:00  28504 - 28836             333         \n",
      "75    2013-10-22 17:00:00  2013-11-19 15:00:00  28837 - 29507             671         \n",
      "76    2013-11-23 03:00:00  2013-12-20 16:00:00  29508 - 30169             662         \n",
      "77    2013-12-20 19:00:00  2013-12-28 19:00:00  30170 - 30362             193         \n",
      "78    2013-12-30 11:00:00  2014-01-12 00:00:00  30363 - 30664             302         \n",
      "79    2014-01-12 06:00:00  2014-01-22 15:00:00  30665 - 30914             250         \n",
      "80    2014-01-22 18:00:00  2014-02-18 16:00:00  30915 - 31561             647         \n",
      "81    2014-02-23 03:00:00  2014-03-21 15:00:00  31562 - 32198             637         \n",
      "82    2014-03-21 17:00:00  2014-04-10 14:00:00  32199 - 32676             478         \n",
      "83    2014-04-14 18:00:00  2014-05-20 10:00:00  32677 - 33533             857         \n",
      "84    2014-05-20 13:00:00  2014-06-04 23:00:00  33534 - 33904             371         \n",
      "85    2014-06-09 11:00:00  2014-06-18 13:00:00  33905 - 34123             219         \n",
      "86    2014-06-18 18:00:00  2014-07-12 12:00:00  34124 - 34694             571         \n",
      "87    2014-07-12 18:00:00  2014-07-22 10:00:00  34695 - 34927             233         \n",
      "88    2014-07-22 13:00:00  2014-08-18 15:00:00  34928 - 35578             651         \n",
      "89    2014-08-21 01:00:00  2014-09-16 06:00:00  35579 - 36208             630         \n",
      "90    2014-09-23 20:00:00  2014-10-20 15:00:00  36209 - 36852             644         \n",
      "91    2014-10-20 17:00:00  2014-11-06 09:00:00  36853 - 37253             401         \n",
      "92    2014-11-06 11:00:00  2014-11-13 16:00:00  37254 - 37427             174         \n",
      "93    2014-11-13 18:00:00  2014-11-20 20:00:00  37428 - 37598             171         \n",
      "94    2014-11-21 09:00:00  2014-12-05 13:00:00  37599 - 37939             341         \n",
      "95    2014-12-09 12:00:00  2014-12-20 07:00:00  37940 - 38199             260         \n",
      "96    2014-12-20 17:00:00  2014-12-31 23:00:00  38200 - 38470             271         \n",
      "==========================================================================================\n",
      "\n",
      "Verifica: Missing PM2.5 nel dataset continuo: 0\n"
     ]
    }
   ],
   "source": [
    "# Estrai dati dalle sequenze continue\n",
    "def extract_continuous_data(dataframe, sequences):\n",
    "    \"\"\"\n",
    "    Estrae i dati dalle sequenze continue e li concatena.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        DataFrame originale\n",
    "    sequences : list of tuples\n",
    "        Lista di sequenze (start_idx, end_idx, length)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame con solo le sequenze continue\n",
    "    list : Lista di tuple (seq_id, start_pos, end_pos) per tracciare le sequenze\n",
    "    \"\"\"\n",
    "    continuous_chunks = []\n",
    "    sequence_info = []\n",
    "    current_position = 0\n",
    "    \n",
    "    for seq_id, (start_idx, end_idx, length) in enumerate(sequences, 1):\n",
    "        chunk = dataframe.iloc[start_idx:end_idx+1].copy()\n",
    "        chunk['sequence_id'] = seq_id  # Traccia da quale sequenza proviene\n",
    "        continuous_chunks.append(chunk)\n",
    "        \n",
    "        # Salva informazioni sulla posizione nel nuovo dataset\n",
    "        sequence_info.append({\n",
    "            'seq_id': seq_id,\n",
    "            'original_start': start_idx,\n",
    "            'original_end': end_idx,\n",
    "            'new_start': current_position,\n",
    "            'new_end': current_position + length - 1,\n",
    "            'length': length,\n",
    "            'date_start': dataframe.index[start_idx],\n",
    "            'date_end': dataframe.index[end_idx]\n",
    "        })\n",
    "        current_position += length\n",
    "    \n",
    "    # Concatena tutte le sequenze\n",
    "    continuous_df = pd.concat(continuous_chunks, axis=0)\n",
    "    \n",
    "    return continuous_df, sequence_info\n",
    "\n",
    "# Estrai dati continui\n",
    "df_continuous, seq_info = extract_continuous_data(df, continuous_sequences)\n",
    "\n",
    "print(\"Dataset Continuo Estratto\\n\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"Shape dataset originale:    {df.shape}\")\n",
    "print(f\"Shape dataset continuo:     {df_continuous.shape}\")\n",
    "print(f\"Righe mantenute:            {len(df_continuous):,} ({len(df_continuous)/len(df)*100:.2f}%)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nInformazioni Sequenze nel Dataset Continuo:\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Seq':<5} {'Inizio Originale':<20} {'Fine Originale':<20} {'Pos. Nuovo Dataset':<25} {'Lunghezza':<12}\")\n",
    "print(\"=\" * 90)\n",
    "for info in seq_info:\n",
    "    pos_range = f\"{info['new_start']} - {info['new_end']}\"\n",
    "    print(f\"{info['seq_id']:<5} {str(info['date_start']):<20} {str(info['date_end']):<20} \"\n",
    "          f\"{pos_range:<25} {info['length']:<12,}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Verifica che non ci siano missing nel dataset continuo\n",
    "missing_in_continuous = df_continuous['pm2.5'].isnull().sum()\n",
    "print(f\"\\nVerifica: Missing PM2.5 nel dataset continuo: {missing_in_continuous}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46aa8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per creare finestre temporali (sliding window)\n",
    "def create_sequences_from_continuous(data, sequence_info, window_size=24, forecast_horizon=1, \n",
    "                                     features=None, target='pm2.5'):\n",
    "    \"\"\"\n",
    "    Crea finestre temporali dalle sequenze continue, rispettando i confini delle sequenze.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        DataFrame con le sequenze continue\n",
    "    sequence_info : list of dict\n",
    "        Informazioni sulle sequenze\n",
    "    window_size : int\n",
    "        Dimensione della finestra di input (numero di timesteps passati)\n",
    "    forecast_horizon : int\n",
    "        Orizzonte di previsione (numero di timesteps futuri da predire)\n",
    "    features : list\n",
    "        Lista di feature da usare (se None, usa tutte tranne target)\n",
    "    target : str\n",
    "        Nome della colonna target\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.array\n",
    "        Array di input shape (n_samples, window_size, n_features)\n",
    "    y : np.array\n",
    "        Array di target shape (n_samples, forecast_horizon)\n",
    "    window_info : list\n",
    "        Informazioni su ogni finestra (per debugging/analisi)\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        # Usa tutte le feature tranne target e sequence_id\n",
    "        features = [col for col in data.columns if col not in [target, 'sequence_id']]\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    window_info = []\n",
    "    \n",
    "    total_windows = 0\n",
    "    \n",
    "    # Processa ogni sequenza separatamente\n",
    "    for seq in sequence_info:\n",
    "        seq_id = seq['seq_id']\n",
    "        start_pos = seq['new_start']\n",
    "        end_pos = seq['new_end']\n",
    "        seq_length = seq['length']\n",
    "        \n",
    "        # Estrai dati della sequenza\n",
    "        seq_data = data.iloc[start_pos:end_pos+1]\n",
    "        \n",
    "        # Calcola quante finestre possiamo creare da questa sequenza\n",
    "        max_windows = seq_length - window_size - forecast_horizon + 1\n",
    "        \n",
    "        if max_windows <= 0:\n",
    "            print(f\"Sequenza {seq_id} troppo corta ({seq_length}) per creare finestre \"\n",
    "                  f\"(necessari almeno {window_size + forecast_horizon})\")\n",
    "            continue\n",
    "        \n",
    "        # Crea finestre dalla sequenza\n",
    "        for i in range(max_windows):\n",
    "            # Finestra di input\n",
    "            window_start = i\n",
    "            window_end = i + window_size\n",
    "            \n",
    "            # Valore/i da predire\n",
    "            target_start = window_end\n",
    "            target_end = window_end + forecast_horizon\n",
    "            \n",
    "            # Estrai dati\n",
    "            X_window = seq_data[features].iloc[window_start:window_end].values\n",
    "            y_window = seq_data[target].iloc[target_start:target_end].values\n",
    "            \n",
    "            # Verifica che non ci siano NaN (controlla solo se numeric)\n",
    "            has_nan = False\n",
    "            try:\n",
    "                has_nan = np.isnan(X_window).any() or np.isnan(y_window).any()\n",
    "            except TypeError:\n",
    "                # Se non è numerico, usa pandas\n",
    "                has_nan = pd.isna(X_window).any() or pd.isna(y_window).any()\n",
    "            \n",
    "            if not has_nan:\n",
    "                X_list.append(X_window)\n",
    "                y_list.append(y_window)\n",
    "                \n",
    "                window_info.append({\n",
    "                    'seq_id': seq_id,\n",
    "                    'window_idx': i,\n",
    "                    'global_start': start_pos + window_start,\n",
    "                    'global_end': start_pos + window_end - 1,\n",
    "                    'target_idx': start_pos + target_start\n",
    "                })\n",
    "                \n",
    "                total_windows += 1\n",
    "        \n",
    "        print(f\"✓ Sequenza {seq_id}: {max_windows:,} finestre create\")\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f\"\\nRiepilogo Windowing:\")\n",
    "    print(f\"   Finestre totali create:  {total_windows:,}\")\n",
    "    print(f\"   X shape:                 {X.shape} (samples, timesteps, features)\")\n",
    "    print(f\"   y shape:                 {y.shape} (samples, forecast_horizon)\")\n",
    "    \n",
    "    return X, y, window_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c579f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri windowing\n",
    "WINDOW_SIZE = 100  # Usa 100 valori precedenti di PM2.5\n",
    "FORECAST_HORIZON = 1  # Predici il prossimo valore\n",
    "\n",
    "feature_columns = ['pm2.5']  # Solo PM2.5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83d9b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sequenza 1: 421 finestre create\n",
      "✓ Sequenza 2: 346 finestre create\n",
      "✓ Sequenza 3: 718 finestre create\n",
      "✓ Sequenza 4: 74 finestre create\n",
      "✓ Sequenza 5: 1,008 finestre create\n",
      "✓ Sequenza 6: 140 finestre create\n",
      "✓ Sequenza 7: 12 finestre create\n",
      "✓ Sequenza 5: 1,008 finestre create\n",
      "✓ Sequenza 6: 140 finestre create\n",
      "✓ Sequenza 7: 12 finestre create\n",
      "✓ Sequenza 8: 155 finestre create\n",
      "✓ Sequenza 9: 1,134 finestre create\n",
      "✓ Sequenza 10: 514 finestre create\n",
      "✓ Sequenza 11: 89 finestre create\n",
      "✓ Sequenza 12: 181 finestre create\n",
      "✓ Sequenza 8: 155 finestre create\n",
      "✓ Sequenza 9: 1,134 finestre create\n",
      "✓ Sequenza 10: 514 finestre create\n",
      "✓ Sequenza 11: 89 finestre create\n",
      "✓ Sequenza 12: 181 finestre create\n",
      "✓ Sequenza 13: 373 finestre create\n",
      "✓ Sequenza 13: 373 finestre create\n",
      "✓ Sequenza 14: 352 finestre create\n",
      "✓ Sequenza 15: 847 finestre create\n",
      "✓ Sequenza 16: 60 finestre create\n",
      "✓ Sequenza 14: 352 finestre create\n",
      "✓ Sequenza 15: 847 finestre create\n",
      "✓ Sequenza 16: 60 finestre create\n",
      "✓ Sequenza 17: 1,224 finestre create\n",
      "✓ Sequenza 18: 112 finestre create\n",
      "✓ Sequenza 19: 63 finestre create\n",
      "✓ Sequenza 20: 94 finestre create\n",
      "✓ Sequenza 17: 1,224 finestre create\n",
      "✓ Sequenza 18: 112 finestre create\n",
      "✓ Sequenza 19: 63 finestre create\n",
      "✓ Sequenza 20: 94 finestre create\n",
      "✓ Sequenza 21: 340 finestre create\n",
      "Sequenza 22 troppo corta (100) per creare finestre (necessari almeno 101)\n",
      "✓ Sequenza 23: 1,230 finestre create\n",
      "✓ Sequenza 24: 146 finestre create\n",
      "✓ Sequenza 25: 25 finestre create\n",
      "✓ Sequenza 21: 340 finestre create\n",
      "Sequenza 22 troppo corta (100) per creare finestre (necessari almeno 101)\n",
      "✓ Sequenza 23: 1,230 finestre create\n",
      "✓ Sequenza 24: 146 finestre create\n",
      "✓ Sequenza 25: 25 finestre create\n",
      "✓ Sequenza 26: 568 finestre create\n",
      "✓ Sequenza 27: 68 finestre create\n",
      "✓ Sequenza 28: 20 finestre create\n",
      "✓ Sequenza 29: 190 finestre create\n",
      "✓ Sequenza 30: 6 finestre create\n",
      "✓ Sequenza 31: 258 finestre create\n",
      "✓ Sequenza 32: 234 finestre create\n",
      "✓ Sequenza 26: 568 finestre create\n",
      "✓ Sequenza 27: 68 finestre create\n",
      "✓ Sequenza 28: 20 finestre create\n",
      "✓ Sequenza 29: 190 finestre create\n",
      "✓ Sequenza 30: 6 finestre create\n",
      "✓ Sequenza 31: 258 finestre create\n",
      "✓ Sequenza 32: 234 finestre create\n",
      "✓ Sequenza 33: 404 finestre create\n",
      "✓ Sequenza 34: 473 finestre create\n",
      "✓ Sequenza 33: 404 finestre create\n",
      "✓ Sequenza 34: 473 finestre create\n",
      "✓ Sequenza 35: 930 finestre create\n",
      "✓ Sequenza 36: 69 finestre create\n",
      "✓ Sequenza 35: 930 finestre create\n",
      "✓ Sequenza 36: 69 finestre create\n",
      "✓ Sequenza 37: 954 finestre create\n",
      "✓ Sequenza 38: 69 finestre create\n",
      "✓ Sequenza 39: 262 finestre create\n",
      "✓ Sequenza 40: 328 finestre create\n",
      "✓ Sequenza 41: 18 finestre create\n",
      "✓ Sequenza 42: 94 finestre create\n",
      "✓ Sequenza 43: 13 finestre create\n",
      "✓ Sequenza 44: 114 finestre create\n",
      "✓ Sequenza 45: 92 finestre create\n",
      "✓ Sequenza 37: 954 finestre create\n",
      "✓ Sequenza 38: 69 finestre create\n",
      "✓ Sequenza 39: 262 finestre create\n",
      "✓ Sequenza 40: 328 finestre create\n",
      "✓ Sequenza 41: 18 finestre create\n",
      "✓ Sequenza 42: 94 finestre create\n",
      "✓ Sequenza 43: 13 finestre create\n",
      "✓ Sequenza 44: 114 finestre create\n",
      "✓ Sequenza 45: 92 finestre create\n",
      "✓ Sequenza 46: 485 finestre create\n",
      "✓ Sequenza 47: 138 finestre create\n",
      "✓ Sequenza 48: 34 finestre create\n",
      "✓ Sequenza 49: 220 finestre create\n",
      "✓ Sequenza 50: 134 finestre create\n",
      "✓ Sequenza 46: 485 finestre create\n",
      "✓ Sequenza 47: 138 finestre create\n",
      "✓ Sequenza 48: 34 finestre create\n",
      "✓ Sequenza 49: 220 finestre create\n",
      "✓ Sequenza 50: 134 finestre create\n",
      "✓ Sequenza 51: 388 finestre create\n",
      "✓ Sequenza 52: 44 finestre create\n",
      "✓ Sequenza 53: 200 finestre create\n",
      "✓ Sequenza 54: 164 finestre create\n",
      "✓ Sequenza 55: 256 finestre create\n",
      "✓ Sequenza 56: 214 finestre create\n",
      "✓ Sequenza 57: 289 finestre create\n",
      "✓ Sequenza 58: 331 finestre create\n",
      "✓ Sequenza 59: 84 finestre create\n",
      "✓ Sequenza 60: 120 finestre create\n",
      "✓ Sequenza 61: 87 finestre create\n",
      "✓ Sequenza 62: 117 finestre create\n",
      "✓ Sequenza 63: 2 finestre create\n",
      "✓ Sequenza 51: 388 finestre create\n",
      "✓ Sequenza 52: 44 finestre create\n",
      "✓ Sequenza 53: 200 finestre create\n",
      "✓ Sequenza 54: 164 finestre create\n",
      "✓ Sequenza 55: 256 finestre create\n",
      "✓ Sequenza 56: 214 finestre create\n",
      "✓ Sequenza 57: 289 finestre create\n",
      "✓ Sequenza 58: 331 finestre create\n",
      "✓ Sequenza 59: 84 finestre create\n",
      "✓ Sequenza 60: 120 finestre create\n",
      "✓ Sequenza 61: 87 finestre create\n",
      "✓ Sequenza 62: 117 finestre create\n",
      "✓ Sequenza 63: 2 finestre create\n",
      "✓ Sequenza 64: 642 finestre create\n",
      "✓ Sequenza 65: 145 finestre create\n",
      "✓ Sequenza 66: 449 finestre create\n",
      "✓ Sequenza 67: 549 finestre create\n",
      "✓ Sequenza 68: 16 finestre create\n",
      "✓ Sequenza 69: 555 finestre create\n",
      "✓ Sequenza 70: 89 finestre create\n",
      "✓ Sequenza 71: 228 finestre create\n",
      "✓ Sequenza 72: 215 finestre create\n",
      "✓ Sequenza 64: 642 finestre create\n",
      "✓ Sequenza 65: 145 finestre create\n",
      "✓ Sequenza 66: 449 finestre create\n",
      "✓ Sequenza 67: 549 finestre create\n",
      "✓ Sequenza 68: 16 finestre create\n",
      "✓ Sequenza 69: 555 finestre create\n",
      "✓ Sequenza 70: 89 finestre create\n",
      "✓ Sequenza 71: 228 finestre create\n",
      "✓ Sequenza 72: 215 finestre create\n",
      "✓ Sequenza 73: 187 finestre create\n",
      "✓ Sequenza 74: 233 finestre create\n",
      "✓ Sequenza 75: 571 finestre create\n",
      "✓ Sequenza 76: 562 finestre create\n",
      "✓ Sequenza 77: 93 finestre create\n",
      "✓ Sequenza 78: 202 finestre create\n",
      "✓ Sequenza 79: 150 finestre create\n",
      "✓ Sequenza 73: 187 finestre create\n",
      "✓ Sequenza 74: 233 finestre create\n",
      "✓ Sequenza 75: 571 finestre create\n",
      "✓ Sequenza 76: 562 finestre create\n",
      "✓ Sequenza 77: 93 finestre create\n",
      "✓ Sequenza 78: 202 finestre create\n",
      "✓ Sequenza 79: 150 finestre create\n",
      "✓ Sequenza 80: 547 finestre create\n",
      "✓ Sequenza 81: 537 finestre create\n",
      "✓ Sequenza 82: 378 finestre create\n",
      "✓ Sequenza 83: 757 finestre create\n",
      "✓ Sequenza 84: 271 finestre create\n",
      "✓ Sequenza 80: 547 finestre create\n",
      "✓ Sequenza 81: 537 finestre create\n",
      "✓ Sequenza 82: 378 finestre create\n",
      "✓ Sequenza 83: 757 finestre create\n",
      "✓ Sequenza 84: 271 finestre create\n",
      "✓ Sequenza 85: 119 finestre create\n",
      "✓ Sequenza 86: 471 finestre create\n",
      "✓ Sequenza 87: 133 finestre create\n",
      "✓ Sequenza 88: 551 finestre create\n",
      "✓ Sequenza 89: 530 finestre create\n",
      "✓ Sequenza 85: 119 finestre create\n",
      "✓ Sequenza 86: 471 finestre create\n",
      "✓ Sequenza 87: 133 finestre create\n",
      "✓ Sequenza 88: 551 finestre create\n",
      "✓ Sequenza 89: 530 finestre create\n",
      "✓ Sequenza 90: 544 finestre create\n",
      "✓ Sequenza 91: 301 finestre create\n",
      "✓ Sequenza 92: 74 finestre create\n",
      "✓ Sequenza 93: 71 finestre create\n",
      "✓ Sequenza 94: 241 finestre create\n",
      "✓ Sequenza 95: 160 finestre create\n",
      "✓ Sequenza 96: 171 finestre create\n",
      "\n",
      "Riepilogo Windowing:\n",
      "   Finestre totali create:  28,871\n",
      "   X shape:                 (28871, 100, 1) (samples, timesteps, features)\n",
      "   y shape:                 (28871, 1) (samples, forecast_horizon)\n",
      "==========================================================================================\n",
      "\n",
      "Shape finali:\n",
      "   X: (28871, 100, 1) → (n_samples=28,871, timesteps=100, features=1)\n",
      "   y: (28871, 1) → (n_samples=28,871, forecast_horizon=1)\n",
      "✓ Sequenza 90: 544 finestre create\n",
      "✓ Sequenza 91: 301 finestre create\n",
      "✓ Sequenza 92: 74 finestre create\n",
      "✓ Sequenza 93: 71 finestre create\n",
      "✓ Sequenza 94: 241 finestre create\n",
      "✓ Sequenza 95: 160 finestre create\n",
      "✓ Sequenza 96: 171 finestre create\n",
      "\n",
      "Riepilogo Windowing:\n",
      "   Finestre totali create:  28,871\n",
      "   X shape:                 (28871, 100, 1) (samples, timesteps, features)\n",
      "   y shape:                 (28871, 1) (samples, forecast_horizon)\n",
      "==========================================================================================\n",
      "\n",
      "Shape finali:\n",
      "   X: (28871, 100, 1) → (n_samples=28,871, timesteps=100, features=1)\n",
      "   y: (28871, 1) → (n_samples=28,871, forecast_horizon=1)\n"
     ]
    }
   ],
   "source": [
    "# Crea finestre temporali\n",
    "X, y, window_info = create_sequences_from_continuous(\n",
    "    data=df_continuous,\n",
    "    sequence_info=seq_info,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    features=['pm2.5'],  # Solo PM2.5 come input!\n",
    "    target='pm2.5'\n",
    ")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nShape finali:\")\n",
    "print(f\"   X: {X.shape} → (n_samples={X.shape[0]:,}, timesteps={X.shape[1]}, features={X.shape[2]})\")\n",
    "print(f\"   y: {y.shape} → (n_samples={y.shape[0]:,}, forecast_horizon={y.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc25ae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assegnazione sequenze in train/val/test:\n",
      "   Train:      49 sequenze → [1, 5, 10, 12, 13, 15, 17, 19, 21, 23, 26, 27, 28, 30, 31, 35, 36, 37, 38, 39, 42, 45, 48, 49, 50, 54, 55, 61, 62, 64, 66, 67, 68, 69, 72, 73, 75, 77, 78, 79, 80, 84, 85, 88, 89, 90, 91, 92, 96]\n",
      "   Validation: 18 sequenze → [3, 6, 9, 14, 20, 25, 29, 44, 52, 53, 56, 57, 60, 71, 74, 76, 81, 87]\n",
      "   Test:       28 sequenze → [2, 4, 7, 8, 11, 16, 18, 24, 32, 33, 34, 40, 41, 43, 46, 47, 51, 58, 59, 63, 65, 70, 82, 83, 86, 93, 94, 95]\n",
      "\n",
      "Percentuali finestre:\n",
      "   Train:      17,340 finestre (60.1%)\n",
      "   Validation: 5,327 finestre (18.5%)\n",
      "   Test:       6,204 finestre (21.5%)\n",
      "\n",
      "Normalizzazione:\n",
      "   X_train_scaled: (17340, 100, 1)\n",
      "   X_val_scaled:   (5327, 100, 1)\n",
      "   X_test_scaled:  (6204, 100, 1)\n",
      "\n",
      "Statistiche target (PM2.5):\n",
      "   Train - min: 2.00, max: 994.00, mean: 95.86\n",
      "   Val   - min: 2.00, max: 700.00, mean: 107.63\n",
      "   Test  - min: 1.00, max: 980.00, mean: 93.80\n"
     ]
    }
   ],
   "source": [
    "# Split Train/Val/Test a livello di sequenze intere\n",
    "# Bilanciato per numero di FINESTRE (non sequenze)\n",
    "# Proporzione: 60% Train / 19% Validation / 21% Test\n",
    "\n",
    "# 1. Conta quante finestre ha ogni sequenza\n",
    "from collections import defaultdict\n",
    "windows_per_sequence = defaultdict(int)\n",
    "\n",
    "for win_info in window_info:\n",
    "    seq_id = win_info['seq_id']\n",
    "    windows_per_sequence[seq_id] += 1\n",
    "\n",
    "total_windows = len(window_info)\n",
    "\n",
    "# 2. Assegnazione greedy delle sequenze per avvicinarsi al 60/20/20\n",
    "target_train_windows = int(0.60 * total_windows)\n",
    "target_val_windows = int(0.18 * total_windows)\n",
    "# Il resto va automaticamente al test\n",
    "\n",
    "# Shuffle casuale delle sequenze\n",
    "np.random.seed(17)\n",
    "sequence_ids = list(windows_per_sequence.keys())\n",
    "np.random.shuffle(sequence_ids)\n",
    "\n",
    "train_sequences = []\n",
    "val_sequences = []\n",
    "test_sequences = []\n",
    "train_window_count = 0\n",
    "val_window_count = 0\n",
    "\n",
    "# Assegna sequenze ai tre split\n",
    "for seq_id in sequence_ids:\n",
    "    if train_window_count < target_train_windows:\n",
    "        # Aggiungi a train\n",
    "        train_sequences.append(seq_id)\n",
    "        train_window_count += windows_per_sequence[seq_id]\n",
    "    elif val_window_count < target_val_windows:\n",
    "        # Aggiungi a validation\n",
    "        val_sequences.append(seq_id)\n",
    "        val_window_count += windows_per_sequence[seq_id]\n",
    "    else:\n",
    "        # Aggiungi a test\n",
    "        test_sequences.append(seq_id)\n",
    "\n",
    "print(f\"\\nAssegnazione sequenze in train/val/test:\")\n",
    "print(f\"   Train:      {len(train_sequences)} sequenze → {sorted(train_sequences)}\")\n",
    "print(f\"   Validation: {len(val_sequences)} sequenze → {sorted(val_sequences)}\")\n",
    "print(f\"   Test:       {len(test_sequences)} sequenze → {sorted(test_sequences)}\")\n",
    "\n",
    "# 3. Raccogli indici delle finestre\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for idx, win_info in enumerate(window_info):\n",
    "    if win_info['seq_id'] in train_sequences:\n",
    "        train_indices.append(idx)\n",
    "    elif win_info['seq_id'] in val_sequences:\n",
    "        val_indices.append(idx)\n",
    "    else:\n",
    "        test_indices.append(idx)\n",
    "\n",
    "# 4. Estrai i dati\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(f\"\\nPercentuali finestre:\")\n",
    "print(f\"   Train:      {len(X_train):,} finestre ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val):,} finestre ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test:       {len(X_test):,} finestre ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Normalizzazione\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reshape per normalizzazione\n",
    "n_train_samples, n_timesteps, n_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "\n",
    "n_val_samples = X_val.shape[0]\n",
    "X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "\n",
    "n_test_samples = X_test.shape[0]\n",
    "X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler solo sul training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape back\n",
    "X_train_scaled = X_train_scaled.reshape(n_train_samples, n_timesteps, n_features)\n",
    "X_val_scaled = X_val_scaled.reshape(n_val_samples, n_timesteps, n_features)\n",
    "X_test_scaled = X_test_scaled.reshape(n_test_samples, n_timesteps, n_features)\n",
    "\n",
    "print(f\"\\nNormalizzazione:\")\n",
    "print(f\"   X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"   X_val_scaled:   {X_val_scaled.shape}\")\n",
    "print(f\"   X_test_scaled:  {X_test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nStatistiche target (PM2.5):\")\n",
    "print(f\"   Train - min: {y_train.min():.2f}, max: {y_train.max():.2f}, mean: {y_train.mean():.2f}\")\n",
    "print(f\"   Val   - min: {y_val.min():.2f}, max: {y_val.max():.2f}, mean: {y_val.mean():.2f}\")\n",
    "print(f\"   Test  - min: {y_test.min():.2f}, max: {y_test.max():.2f}, mean: {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad874d40",
   "metadata": {},
   "source": [
    "# GRU Model with Hyperparameter Optimization\n",
    "\n",
    "Implementazione di **Gated Recurrent Unit (GRU)** per previsione PM2.5 con ottimizzazione Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff36922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna version: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "# Installa Optuna se necessario\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.pruners import MedianPruner\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\", \"-q\"])\n",
    "    import optuna\n",
    "    from optuna.pruners import MedianPruner\n",
    "\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6a8c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths configurati:\n",
      "   GRU model: gru_pm25_forecaster_fixed_dropout.h5\n",
      "   GRU config: gru_best_params_fixed_dropout.pkl\n",
      "   GRU history: gru_training_history_fixed_dropout.pkl\n"
     ]
    }
   ],
   "source": [
    "# Configura paths per salvataggio modelli\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "gru_model_path = models_dir / 'gru_pm25_forecaster_fixed_dropout.h5'\n",
    "gru_config_path = models_dir / 'gru_best_params_fixed_dropout.pkl'\n",
    "gru_history_path = models_dir / 'gru_training_history_fixed_dropout.pkl'\n",
    "\n",
    "print(f\"Paths configurati:\")\n",
    "print(f\"   GRU model: {gru_model_path.name}\")\n",
    "print(f\"   GRU config: {gru_config_path.name}\")\n",
    "print(f\"   GRU history: {gru_history_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e13af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(units, dropout, learning_rate, n_layers, input_shape):\n",
    "    \"\"\"\n",
    "    Crea modello GRU per time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    units : int\n",
    "        Numero di unità GRU\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    learning_rate : float\n",
    "        Learning rate per Adam optimizer\n",
    "    n_layers : int\n",
    "        Numero di layer GRU da impilare\n",
    "    input_shape : tuple\n",
    "        (timesteps, features)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Modello GRU compilato\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Primo layer\n",
    "    if n_layers == 1:\n",
    "        model.add(keras.layers.GRU(units, dropout=dropout, input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(keras.layers.GRU(units, dropout=dropout, return_sequences=True, input_shape=input_shape))\n",
    "        \n",
    "        # Layer intermedi\n",
    "        for i in range(1, n_layers - 1):\n",
    "            model.add(keras.layers.GRU(units, dropout=dropout, return_sequences=True))\n",
    "        \n",
    "        # Ultimo layer\n",
    "        model.add(keras.layers.GRU(units, dropout=dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca8dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function per Optuna - GRU\n",
    "    Griglia ridotta per velocità\n",
    "    \"\"\"\n",
    "    # Spazio di ricerca LIMITATO\n",
    "    params = {\n",
    "        'units': trial.suggest_categorical('units', [32, 64, 128]),\n",
    "        'dropout': 0.1,  # Fisso a 0.1\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [1e-3, 5e-4]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        'n_layers': trial.suggest_int('n_layers', 1, 3)  # 1-3 layer GRU\n",
    "    }\n",
    "    \n",
    "    # Crea modello\n",
    "    model = create_gru_model(\n",
    "        units=params['units'],\n",
    "        dropout=params['dropout'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_layers=params['n_layers'],\n",
    "        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Training veloce (max 30 epochs)\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Ritorna best validation loss\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e473930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:18:54,391] A new study created in memory with name: gru_pm25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRU: OTTIMIZZAZIONE HYPERPARAMETERS\n",
      "======================================================================\n",
      "   • Trials: 15\n",
      "   • Max epochs per trial: 50\n",
      "   • Early stopping: patience=5\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1081.4:   7%|▋         | 1/15 [03:53<54:35, 233.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:22:48,378] Trial 0 finished with value: 1081.4000244140625 and parameters: {'units': 32, 'learning_rate': 0.0005, 'batch_size': 32, 'n_layers': 1}. Best is trial 0 with value: 1081.4000244140625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 699.774:  13%|█▎        | 2/15 [09:35<1:04:23, 297.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:28:29,880] Trial 1 finished with value: 699.77392578125 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 32, 'n_layers': 1}. Best is trial 1 with value: 699.77392578125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 699.774:  20%|██        | 3/15 [13:52<55:47, 278.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:32:47,122] Trial 2 finished with value: 1102.5950927734375 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 1}. Best is trial 1 with value: 699.77392578125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  27%|██▋       | 4/15 [24:28<1:16:58, 419.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:43:23,021] Trial 3 finished with value: 620.9668579101562 and parameters: {'units': 128, 'learning_rate': 0.0005, 'batch_size': 32, 'n_layers': 1}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  33%|███▎      | 5/15 [50:51<2:19:54, 839.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 02:09:46,307] Trial 4 finished with value: 698.4905395507812 and parameters: {'units': 128, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 2}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  40%|████      | 6/15 [56:34<1:40:35, 670.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 02:15:29,100] Trial 5 finished with value: 706.7031860351562 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 32, 'n_layers': 1}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  47%|████▋     | 7/15 [1:00:45<1:11:07, 533.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 02:19:40,136] Trial 6 finished with value: 1088.6524658203125 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 1}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  53%|█████▎    | 8/15 [1:40:27<2:10:53, 1121.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 02:59:22,026] Trial 7 finished with value: 830.1353759765625 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 3}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  60%|██████    | 9/15 [1:46:19<1:28:07, 881.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 03:05:14,285] Trial 8 finished with value: 3196.869873046875 and parameters: {'units': 32, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 2}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  67%|██████▋   | 10/15 [1:56:56<1:07:08, 805.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 03:15:50,762] Trial 9 finished with value: 711.2557983398438 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 1}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  73%|███████▎  | 11/15 [2:23:19<1:09:34, 1043.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 03:42:14,034] Trial 10 finished with value: 643.4942016601562 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 32, 'n_layers': 3}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  80%|████████  | 12/15 [2:49:16<59:59, 1199.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 04:08:10,893] Trial 11 finished with value: 673.1396484375 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 32, 'n_layers': 3}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  87%|████████▋ | 13/15 [2:59:39<34:09, 1024.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 04:18:33,527] Trial 12 finished with value: 9380.365234375 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 32, 'n_layers': 3}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 620.967:  93%|█████████▎| 14/15 [3:16:18<16:57, 1017.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 04:35:13,290] Trial 13 finished with value: 663.9059448242188 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 32, 'n_layers': 2}. Best is trial 3 with value: 620.9668579101562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 612.485: 100%|██████████| 15/15 [3:57:15<00:00, 949.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 05:16:09,676] Trial 14 finished with value: 612.485107421875 and parameters: {'units': 128, 'learning_rate': 0.001, 'batch_size': 32, 'n_layers': 3}. Best is trial 14 with value: 612.485107421875.\n",
      "\n",
      "======================================================================\n",
      "GRU: OTTIMIZZAZIONE COMPLETATA\n",
      "======================================================================\n",
      "\n",
      "Best validation loss: 612.4851\n",
      "\n",
      "Best hyperparameters:\n",
      "   • units: 128\n",
      "   • learning_rate: 0.001\n",
      "   • batch_size: 32\n",
      "   • n_layers: 3\n",
      "\n",
      "Best params salvati: gru_best_params_fixed_dropout.pkl\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ottimizzazione GRU con Optuna (15 trials)\n",
    "\n",
    "import pickle\n",
    "\n",
    "if gru_config_path.exists():\n",
    "    print(\"=\"*70)\n",
    "    print(\"GRU: Best params esistenti - CARICAMENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(gru_config_path, 'rb') as f:\n",
    "        gru_best_params = pickle.load(f)\n",
    "    \n",
    "    print(\"\\nGRU Best hyperparameters:\")\n",
    "    for key, value in gru_best_params.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"GRU: OTTIMIZZAZIONE HYPERPARAMETERS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   • Trials: 15\")\n",
    "    print(f\"   • Max epochs per trial: 50\")\n",
    "    print(f\"   • Early stopping: patience=5\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Crea study\n",
    "    study = optuna.create_study(\n",
    "        study_name='gru_pm25',\n",
    "        direction='minimize',\n",
    "        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    # Ottimizza (15 trials)\n",
    "    study.optimize(\n",
    "        gru_objective,\n",
    "        n_trials=15,\n",
    "        timeout=None,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GRU: OTTIMIZZAZIONE COMPLETATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gru_best_params = study.best_trial.params\n",
    "    best_val_loss = study.best_trial.value\n",
    "    \n",
    "    print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in gru_best_params.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    # Salva best params\n",
    "    with open(gru_config_path, 'wb') as f:\n",
    "        pickle.dump(gru_best_params, f)\n",
    "    \n",
    "    print(f\"\\nBest params salvati: {gru_config_path.name}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d45646ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRU: TRAINING FINALE CON BEST PARAMS\n",
      "======================================================================\n",
      "\n",
      "GRU Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_28 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m50,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_29 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m99,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_30 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m99,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">248,577</span> (971.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m248,577\u001b[0m (971.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">248,577</span> (971.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m248,577\u001b[0m (971.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INIZIO TRAINING\n",
      "======================================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 106ms/step - loss: 11081.5811 - mae: 69.5571 - val_loss: 10745.8848 - val_mae: 71.6679 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 106ms/step - loss: 11081.5811 - mae: 69.5571 - val_loss: 10745.8848 - val_mae: 71.6679 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 109ms/step - loss: 8372.7998 - mae: 65.4563 - val_loss: 9559.2959 - val_mae: 71.3809 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 109ms/step - loss: 8372.7998 - mae: 65.4563 - val_loss: 9559.2959 - val_mae: 71.3809 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 8153.3193 - mae: 67.4123 - val_loss: 9422.0557 - val_mae: 71.8745 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 8153.3193 - mae: 67.4123 - val_loss: 9422.0557 - val_mae: 71.8745 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 5699.9053 - mae: 44.0040 - val_loss: 4565.7002 - val_mae: 34.0848 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 5699.9053 - mae: 44.0040 - val_loss: 4565.7002 - val_mae: 34.0848 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 3527.4915 - mae: 29.8836 - val_loss: 2993.4795 - val_mae: 26.1225 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 3527.4915 - mae: 29.8836 - val_loss: 2993.4795 - val_mae: 26.1225 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 2677.4343 - mae: 25.8779 - val_loss: 2195.9500 - val_mae: 22.6318 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 2677.4343 - mae: 25.8779 - val_loss: 2195.9500 - val_mae: 22.6318 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 2265.1033 - mae: 24.0236 - val_loss: 1733.3345 - val_mae: 20.9415 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 2265.1033 - mae: 24.0236 - val_loss: 1733.3345 - val_mae: 20.9415 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 2009.3610 - mae: 22.3063 - val_loss: 1436.9116 - val_mae: 19.6928 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 2009.3610 - mae: 22.3063 - val_loss: 1436.9116 - val_mae: 19.6928 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 1754.0519 - mae: 21.5594 - val_loss: 1266.8129 - val_mae: 19.9016 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 1754.0519 - mae: 21.5594 - val_loss: 1266.8129 - val_mae: 19.9016 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 115ms/step - loss: 1700.8330 - mae: 21.0743 - val_loss: 1091.6163 - val_mae: 17.7274 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 115ms/step - loss: 1700.8330 - mae: 21.0743 - val_loss: 1091.6163 - val_mae: 17.7274 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1578.2856 - mae: 20.4306 - val_loss: 932.1510 - val_mae: 16.4518 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1578.2856 - mae: 20.4306 - val_loss: 932.1510 - val_mae: 16.4518 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1469.2351 - mae: 19.8362 - val_loss: 938.8914 - val_mae: 18.2437 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1469.2351 - mae: 19.8362 - val_loss: 938.8914 - val_mae: 18.2437 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 118ms/step - loss: 1432.0492 - mae: 19.6412 - val_loss: 788.2682 - val_mae: 15.6073 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 118ms/step - loss: 1432.0492 - mae: 19.6412 - val_loss: 788.2682 - val_mae: 15.6073 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 118ms/step - loss: 1425.8091 - mae: 19.4167 - val_loss: 792.5911 - val_mae: 17.1656 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 118ms/step - loss: 1425.8091 - mae: 19.4167 - val_loss: 792.5911 - val_mae: 17.1656 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - loss: 1448.9735 - mae: 19.4232 - val_loss: 766.2062 - val_mae: 16.7534 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - loss: 1448.9735 - mae: 19.4232 - val_loss: 766.2062 - val_mae: 16.7534 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 121ms/step - loss: 1288.6021 - mae: 18.7769 - val_loss: 708.3613 - val_mae: 15.8488 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 121ms/step - loss: 1288.6021 - mae: 18.7769 - val_loss: 708.3613 - val_mae: 15.8488 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 115ms/step - loss: 1310.5049 - mae: 18.8641 - val_loss: 801.3840 - val_mae: 16.8378 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 115ms/step - loss: 1310.5049 - mae: 18.8641 - val_loss: 801.3840 - val_mae: 16.8378 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1360.4777 - mae: 19.0997 - val_loss: 726.4761 - val_mae: 17.2982 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1360.4777 - mae: 19.0997 - val_loss: 726.4761 - val_mae: 17.2982 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1264.8950 - mae: 18.5634 - val_loss: 699.4462 - val_mae: 16.1798 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1264.8950 - mae: 18.5634 - val_loss: 699.4462 - val_mae: 16.1798 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1403.2971 - mae: 19.0249 - val_loss: 738.9481 - val_mae: 17.2919 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1403.2971 - mae: 19.0249 - val_loss: 738.9481 - val_mae: 17.2919 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - loss: 1290.1942 - mae: 18.4137 - val_loss: 649.6743 - val_mae: 15.2151 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - loss: 1290.1942 - mae: 18.4137 - val_loss: 649.6743 - val_mae: 15.2151 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 119ms/step - loss: 1342.7777 - mae: 18.8465 - val_loss: 700.4323 - val_mae: 17.3992 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 119ms/step - loss: 1342.7777 - mae: 18.8465 - val_loss: 700.4323 - val_mae: 17.3992 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1301.3766 - mae: 18.6101 - val_loss: 654.6729 - val_mae: 15.2115 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1301.3766 - mae: 18.6101 - val_loss: 654.6729 - val_mae: 15.2115 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1352.3358 - mae: 18.7378 - val_loss: 724.9212 - val_mae: 16.7554 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1352.3358 - mae: 18.7378 - val_loss: 724.9212 - val_mae: 16.7554 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 1265.4646 - mae: 18.4553 - val_loss: 630.8669 - val_mae: 15.2668 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 1265.4646 - mae: 18.4553 - val_loss: 630.8669 - val_mae: 15.2668 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 110ms/step - loss: 1284.6627 - mae: 18.5799 - val_loss: 679.0007 - val_mae: 16.2121 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 110ms/step - loss: 1284.6627 - mae: 18.5799 - val_loss: 679.0007 - val_mae: 16.2121 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 1255.6842 - mae: 18.2894 - val_loss: 664.0328 - val_mae: 16.0222 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 112ms/step - loss: 1255.6842 - mae: 18.2894 - val_loss: 664.0328 - val_mae: 16.0222 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1184.0815 - mae: 17.9893 - val_loss: 622.6762 - val_mae: 14.9086 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1184.0815 - mae: 17.9893 - val_loss: 622.6762 - val_mae: 14.9086 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1305.6238 - mae: 18.4333 - val_loss: 671.6425 - val_mae: 16.2160 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1305.6238 - mae: 18.4333 - val_loss: 671.6425 - val_mae: 16.2160 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1256.9866 - mae: 18.2244 - val_loss: 656.6187 - val_mae: 15.4457 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1256.9866 - mae: 18.2244 - val_loss: 656.6187 - val_mae: 15.4457 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 117ms/step - loss: 1261.9362 - mae: 18.2358 - val_loss: 668.3506 - val_mae: 16.3604 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 117ms/step - loss: 1261.9362 - mae: 18.2358 - val_loss: 668.3506 - val_mae: 16.3604 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 118ms/step - loss: 1202.5852 - mae: 17.9724 - val_loss: 747.5221 - val_mae: 16.6194 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 118ms/step - loss: 1202.5852 - mae: 17.9724 - val_loss: 747.5221 - val_mae: 16.6194 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - loss: 1171.9421 - mae: 17.9935 - val_loss: 645.0428 - val_mae: 15.6076 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - loss: 1171.9421 - mae: 17.9935 - val_loss: 645.0428 - val_mae: 15.6076 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1248.4180 - mae: 18.0430 - val_loss: 703.0868 - val_mae: 16.8111 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1248.4180 - mae: 18.0430 - val_loss: 703.0868 - val_mae: 16.8111 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 1315.1439 - mae: 18.2081 - val_loss: 762.1692 - val_mae: 16.8979 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 1315.1439 - mae: 18.2081 - val_loss: 762.1692 - val_mae: 16.8979 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1151.2913 - mae: 17.6058 - val_loss: 657.8068 - val_mae: 16.4118 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1151.2913 - mae: 17.6058 - val_loss: 657.8068 - val_mae: 16.4118 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 115ms/step - loss: 1205.2816 - mae: 17.8087 - val_loss: 660.6138 - val_mae: 15.9195 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 115ms/step - loss: 1205.2816 - mae: 17.8087 - val_loss: 660.6138 - val_mae: 15.9195 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1191.9713 - mae: 17.7548 - val_loss: 645.4348 - val_mae: 15.5814 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1191.9713 - mae: 17.7548 - val_loss: 645.4348 - val_mae: 15.5814 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 1181.0287 - mae: 17.4127 - val_loss: 650.9301 - val_mae: 15.8475 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 116ms/step - loss: 1181.0287 - mae: 17.4127 - val_loss: 650.9301 - val_mae: 15.8475 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1217.8507 - mae: 17.6810 - val_loss: 741.2274 - val_mae: 17.8279 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 114ms/step - loss: 1217.8507 - mae: 17.6810 - val_loss: 741.2274 - val_mae: 17.8279 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 1153.7434 - mae: 17.4255 - val_loss: 680.8981 - val_mae: 16.1065 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 111ms/step - loss: 1153.7434 - mae: 17.4255 - val_loss: 680.8981 - val_mae: 16.1065 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1208.2128 - mae: 17.8449 - val_loss: 741.7239 - val_mae: 16.7235 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1208.2128 - mae: 17.8449 - val_loss: 741.7239 - val_mae: 16.7235 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1122.3228 - mae: 17.2341 - val_loss: 645.6425 - val_mae: 15.8449 - learning_rate: 2.5000e-04\n",
      "\u001b[1m542/542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 113ms/step - loss: 1122.3228 - mae: 17.2341 - val_loss: 645.6425 - val_mae: 15.8449 - learning_rate: 2.5000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GRU: TRAINING COMPLETATO\n",
      "======================================================================\n",
      "Modello salvato: gru_pm25_forecaster_fixed_dropout.h5\n",
      "History salvata: gru_training_history_fixed_dropout.pkl\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training finale GRU con best params\n",
    "\n",
    "if gru_model_path.exists():\n",
    "    print(\"=\"*70)\n",
    "    print(\"GRU: Modello finale esistente - CARICAMENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gru_model = keras.models.load_model(gru_model_path)\n",
    "    \n",
    "    with open(gru_history_path, 'rb') as f:\n",
    "        gru_history = pickle.load(f)\n",
    "    \n",
    "    print(f\"Modello caricato da: {gru_model_path.name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"GRU: TRAINING FINALE CON BEST PARAMS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crea modello finale\n",
    "    gru_model = create_gru_model(\n",
    "        units=gru_best_params['units'],\n",
    "        dropout=0.1,#gru_best_params['dropout'],\n",
    "        learning_rate=gru_best_params['learning_rate'],\n",
    "        n_layers=gru_best_params['n_layers'],\n",
    "        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGRU Model Summary:\")\n",
    "    gru_model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Training completo\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INIZIO TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    history = gru_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=gru_best_params['batch_size'],\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Salva modello e history\n",
    "    gru_model.save(gru_model_path)\n",
    "    \n",
    "    gru_history = history.history\n",
    "    with open(gru_history_path, 'wb') as f:\n",
    "        pickle.dump(gru_history, f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GRU: TRAINING COMPLETATO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Modello salvato: {gru_model_path.name}\")\n",
    "    print(f\"History salvata: {gru_history_path.name}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ad1f6",
   "metadata": {},
   "source": [
    "# Simple RNN Model with Hyperparameter Optimization\n",
    "\n",
    "Implementazione di **Simple RNN** per confronto con GRU e LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de04d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths RNN configurati:\n",
      "   RNN model: rnn_pm25_forecaster_fixed_dropout.h5\n",
      "   RNN config: rnn_best_params_fixed_dropout.pkl\n",
      "   RNN history: rnn_training_history_fixed_dropout.pkl\n"
     ]
    }
   ],
   "source": [
    "# Configura paths per RNN\n",
    "rnn_model_path = models_dir / 'rnn_pm25_forecaster_fixed_dropout.h5'\n",
    "rnn_config_path = models_dir / 'rnn_best_params_fixed_dropout.pkl'\n",
    "rnn_history_path = models_dir / 'rnn_training_history_fixed_dropout.pkl'\n",
    "\n",
    "print(f\"Paths RNN configurati:\")\n",
    "print(f\"   RNN model: {rnn_model_path.name}\")\n",
    "print(f\"   RNN config: {rnn_config_path.name}\")\n",
    "print(f\"   RNN history: {rnn_history_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8693b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Funzione create_rnn_model definita\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_model(units, dropout, learning_rate, n_layers, input_shape):\n",
    "    \"\"\"\n",
    "    Crea modello Simple RNN per time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    units : int\n",
    "        Numero di unità RNN\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    learning_rate : float\n",
    "        Learning rate per Adam optimizer\n",
    "    n_layers : int\n",
    "        Numero di layer RNN da impilare\n",
    "    input_shape : tuple\n",
    "        (timesteps, features)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Modello RNN compilato\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Primo layer\n",
    "    if n_layers == 1:\n",
    "        model.add(keras.layers.SimpleRNN(units, dropout=dropout, input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(keras.layers.SimpleRNN(units, dropout=dropout, return_sequences=True, input_shape=input_shape))\n",
    "        \n",
    "        # Layer intermedi\n",
    "        for i in range(1, n_layers - 1):\n",
    "            model.add(keras.layers.SimpleRNN(units, dropout=dropout, return_sequences=True))\n",
    "        \n",
    "        # Ultimo layer\n",
    "        model.add(keras.layers.SimpleRNN(units, dropout=dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Funzione create_rnn_model definita\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "581fc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function per Optuna - Simple RNN\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'units': trial.suggest_categorical('units', [32, 64]),\n",
    "        'dropout': 0.1,  # Fisso a 0.1\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [1e-3, 5e-4]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128]),\n",
    "        'n_layers': trial.suggest_int('n_layers', 1, 3)  # 1-3 layer RNN\n",
    "    }\n",
    "    \n",
    "    model = create_rnn_model(\n",
    "        units=params['units'],\n",
    "        dropout=params['dropout'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_layers=params['n_layers'],\n",
    "        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    )\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f95d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:44:09,205] A new study created in memory with name: rnn_pm25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RNN: OTTIMIZZAZIONE HYPERPARAMETERS\n",
      "======================================================================\n",
      "   • Trials: 15\n",
      "   • Max epochs per trial: 50\n",
      "   • Early stopping: patience=5\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1389.41:   7%|▋         | 1/15 [00:57<13:23, 57.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:45:06,614] Trial 0 finished with value: 1389.4114990234375 and parameters: {'units': 64, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 1}. Best is trial 0 with value: 1389.4114990234375.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1389.41:  13%|█▎        | 2/15 [03:22<23:36, 108.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:47:31,652] Trial 1 finished with value: 3293.300537109375 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 3}. Best is trial 0 with value: 1389.4114990234375.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 1082.25:  20%|██        | 3/15 [04:16<16:47, 83.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:48:25,828] Trial 2 finished with value: 1082.2452392578125 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 2 with value: 1082.2452392578125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 1082.25:  27%|██▋       | 4/15 [10:19<35:33, 193.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:54:28,367] Trial 3 finished with value: 1144.523681640625 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 3}. Best is trial 2 with value: 1082.2452392578125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  33%|███▎      | 5/15 [13:56<33:44, 202.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:58:06,028] Trial 4 finished with value: 718.8506469726562 and parameters: {'units': 64, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 2}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  40%|████      | 6/15 [15:25<24:33, 163.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 09:59:34,563] Trial 5 finished with value: 6844.728515625 and parameters: {'units': 32, 'learning_rate': 0.0005, 'batch_size': 128, 'n_layers': 2}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  47%|████▋     | 7/15 [16:20<17:06, 128.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:00:29,991] Trial 6 finished with value: 1282.133056640625 and parameters: {'units': 64, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 1}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  53%|█████▎    | 8/15 [17:48<13:27, 115.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:01:57,445] Trial 7 finished with value: 3121.2890625 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 2}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  60%|██████    | 9/15 [18:41<09:35, 95.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:02:50,581] Trial 8 finished with value: 1100.6588134765625 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  67%|██████▋   | 10/15 [21:05<09:14, 110.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:05:15,001] Trial 9 finished with value: 3181.005859375 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 3}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 718.851:  73%|███████▎  | 11/15 [24:48<09:40, 145.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:08:58,075] Trial 10 finished with value: 1098.8438720703125 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 2}. Best is trial 4 with value: 718.8506469726562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 709.903:  80%|████████  | 12/15 [26:05<06:12, 124.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:10:14,516] Trial 11 finished with value: 709.9029541015625 and parameters: {'units': 64, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 11 with value: 709.9029541015625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 709.903:  87%|████████▋ | 13/15 [29:44<05:05, 152.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:13:53,364] Trial 12 finished with value: 719.4736938476562 and parameters: {'units': 64, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 2}. Best is trial 11 with value: 709.9029541015625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 683.971:  93%|█████████▎| 14/15 [31:04<02:10, 131.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:15:13,678] Trial 13 finished with value: 683.9708862304688 and parameters: {'units': 64, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 13 with value: 683.9708862304688.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 683.971: 100%|██████████| 15/15 [32:22<00:00, 129.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:16:32,119] Trial 14 finished with value: 1991.0498046875 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 1}. Best is trial 13 with value: 683.9708862304688.\n",
      "\n",
      "======================================================================\n",
      "RNN: OTTIMIZZAZIONE COMPLETATA\n",
      "======================================================================\n",
      "\n",
      "Best validation loss: 683.9709\n",
      "\n",
      "Best hyperparameters:\n",
      "   • units: 64\n",
      "   • learning_rate: 0.001\n",
      "   • batch_size: 64\n",
      "   • n_layers: 1\n",
      "\n",
      "✓ Best params salvati: rnn_best_params_fixed_dropout.pkl\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ottimizzazione RNN con Optuna (15 trials)\n",
    "\n",
    "if rnn_config_path.exists():\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ RNN: Best params esistenti - CARICAMENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(rnn_config_path, 'rb') as f:\n",
    "        rnn_best_params = pickle.load(f)\n",
    "    \n",
    "    print(\"\\nRNN Best hyperparameters:\")\n",
    "    for key, value in rnn_best_params.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"RNN: OTTIMIZZAZIONE HYPERPARAMETERS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   • Trials: 15\")\n",
    "    print(f\"   • Max epochs per trial: 50\")\n",
    "    print(f\"   • Early stopping: patience=5\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name='rnn_pm25',\n",
    "        direction='minimize',\n",
    "        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        rnn_objective,\n",
    "        n_trials=15,\n",
    "        timeout=None,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RNN: OTTIMIZZAZIONE COMPLETATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rnn_best_params = study.best_trial.params\n",
    "    best_val_loss = study.best_trial.value\n",
    "    \n",
    "    print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in rnn_best_params.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    with open(rnn_config_path, 'wb') as f:\n",
    "        pickle.dump(rnn_best_params, f)\n",
    "    \n",
    "    print(f\"\\n✓ Best params salvati: {rnn_config_path.name}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b6f9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RNN: TRAINING FINALE CON BEST PARAMS\n",
      "======================================================================\n",
      "\n",
      "RNN Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_31\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_31\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_26 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,289</span> (16.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,289\u001b[0m (16.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,289</span> (16.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,289\u001b[0m (16.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INIZIO TRAINING\n",
      "======================================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 15122.2344 - mae: 84.6224 - val_loss: 16260.2441 - val_mae: 87.6136 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 15122.2344 - mae: 84.6224 - val_loss: 16260.2441 - val_mae: 87.6136 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 12804.1084 - mae: 75.3058 - val_loss: 14145.3730 - val_mae: 80.3673 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 12804.1084 - mae: 75.3058 - val_loss: 14145.3730 - val_mae: 80.3673 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 10776.4854 - mae: 67.6374 - val_loss: 14321.9912 - val_mae: 80.6229 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 10776.4854 - mae: 67.6374 - val_loss: 14321.9912 - val_mae: 80.6229 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 9687.4521 - mae: 65.1174 - val_loss: 11113.0225 - val_mae: 72.1600 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 9687.4521 - mae: 65.1174 - val_loss: 11113.0225 - val_mae: 72.1600 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8775.3594 - mae: 63.6663 - val_loss: 10239.0889 - val_mae: 70.1946 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8775.3594 - mae: 63.6663 - val_loss: 10239.0889 - val_mae: 70.1946 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7863.8569 - mae: 57.5603 - val_loss: 8731.1885 - val_mae: 59.8562 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7863.8569 - mae: 57.5603 - val_loss: 8731.1885 - val_mae: 59.8562 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6693.1880 - mae: 49.3181 - val_loss: 7617.2983 - val_mae: 54.0810 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6693.1880 - mae: 49.3181 - val_loss: 7617.2983 - val_mae: 54.0810 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6075.8599 - mae: 46.2167 - val_loss: 6739.1738 - val_mae: 49.4859 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6075.8599 - mae: 46.2167 - val_loss: 6739.1738 - val_mae: 49.4859 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5475.3955 - mae: 42.9649 - val_loss: 5968.6670 - val_mae: 45.0833 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5475.3955 - mae: 42.9649 - val_loss: 5968.6670 - val_mae: 45.0833 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4917.5679 - mae: 39.9061 - val_loss: 5327.3955 - val_mae: 42.1826 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4917.5679 - mae: 39.9061 - val_loss: 5327.3955 - val_mae: 42.1826 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4517.7778 - mae: 37.7250 - val_loss: 4762.7002 - val_mae: 38.6909 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4517.7778 - mae: 37.7250 - val_loss: 4762.7002 - val_mae: 38.6909 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4168.9463 - mae: 35.5808 - val_loss: 4464.3560 - val_mae: 37.9461 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4168.9463 - mae: 35.5808 - val_loss: 4464.3560 - val_mae: 37.9461 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7135.4604 - mae: 58.3506 - val_loss: 8308.5117 - val_mae: 63.3811 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7135.4604 - mae: 58.3506 - val_loss: 8308.5117 - val_mae: 63.3811 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4405.4062 - mae: 38.9336 - val_loss: 3990.4341 - val_mae: 36.6402 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4405.4062 - mae: 38.9336 - val_loss: 3990.4341 - val_mae: 36.6402 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3569.6685 - mae: 33.6460 - val_loss: 3648.0579 - val_mae: 34.6246 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3569.6685 - mae: 33.6460 - val_loss: 3648.0579 - val_mae: 34.6246 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3301.9009 - mae: 31.9537 - val_loss: 3232.4629 - val_mae: 32.4825 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3301.9009 - mae: 31.9537 - val_loss: 3232.4629 - val_mae: 32.4825 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3056.1294 - mae: 30.2566 - val_loss: 2912.5222 - val_mae: 29.9189 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3056.1294 - mae: 30.2566 - val_loss: 2912.5222 - val_mae: 29.9189 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2809.6736 - mae: 28.5706 - val_loss: 2681.6389 - val_mae: 28.3440 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2809.6736 - mae: 28.5706 - val_loss: 2681.6389 - val_mae: 28.3440 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2683.7510 - mae: 27.5126 - val_loss: 2491.2620 - val_mae: 27.5145 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2683.7510 - mae: 27.5126 - val_loss: 2491.2620 - val_mae: 27.5145 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2527.8599 - mae: 26.4986 - val_loss: 2508.0464 - val_mae: 26.8330 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2527.8599 - mae: 26.4986 - val_loss: 2508.0464 - val_mae: 26.8330 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2444.6257 - mae: 26.1453 - val_loss: 2083.1699 - val_mae: 24.7996 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2444.6257 - mae: 26.1453 - val_loss: 2083.1699 - val_mae: 24.7996 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2308.4536 - mae: 25.4942 - val_loss: 2003.9758 - val_mae: 24.3215 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2308.4536 - mae: 25.4942 - val_loss: 2003.9758 - val_mae: 24.3215 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2159.0308 - mae: 24.4332 - val_loss: 1794.5763 - val_mae: 23.1268 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2159.0308 - mae: 24.4332 - val_loss: 1794.5763 - val_mae: 23.1268 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2190.5154 - mae: 24.6520 - val_loss: 1730.4084 - val_mae: 22.6209 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2190.5154 - mae: 24.6520 - val_loss: 1730.4084 - val_mae: 22.6209 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2276.9163 - mae: 26.6606 - val_loss: 1868.7435 - val_mae: 26.0729 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2276.9163 - mae: 26.6606 - val_loss: 1868.7435 - val_mae: 26.0729 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1979.7988 - mae: 24.3526 - val_loss: 1564.2880 - val_mae: 22.9238 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1979.7988 - mae: 24.3526 - val_loss: 1564.2880 - val_mae: 22.9238 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1873.0168 - mae: 23.1591 - val_loss: 1436.2491 - val_mae: 22.0370 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1873.0168 - mae: 23.1591 - val_loss: 1436.2491 - val_mae: 22.0370 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1788.4780 - mae: 22.3212 - val_loss: 1346.4104 - val_mae: 21.6287 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1788.4780 - mae: 22.3212 - val_loss: 1346.4104 - val_mae: 21.6287 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1737.9603 - mae: 21.8923 - val_loss: 1293.5970 - val_mae: 20.7953 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1737.9603 - mae: 21.8923 - val_loss: 1293.5970 - val_mae: 20.7953 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1670.0151 - mae: 21.1113 - val_loss: 1200.4767 - val_mae: 19.7364 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1670.0151 - mae: 21.1113 - val_loss: 1200.4767 - val_mae: 19.7364 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1615.8079 - mae: 21.0192 - val_loss: 1121.7279 - val_mae: 19.3565 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1615.8079 - mae: 21.0192 - val_loss: 1121.7279 - val_mae: 19.3565 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1641.2466 - mae: 20.7098 - val_loss: 1089.5601 - val_mae: 19.2471 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1641.2466 - mae: 20.7098 - val_loss: 1089.5601 - val_mae: 19.2471 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1580.7628 - mae: 20.9040 - val_loss: 1031.0222 - val_mae: 18.4836 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1580.7628 - mae: 20.9040 - val_loss: 1031.0222 - val_mae: 18.4836 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1482.9730 - mae: 19.8733 - val_loss: 1017.8853 - val_mae: 18.7190 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1482.9730 - mae: 19.8733 - val_loss: 1017.8853 - val_mae: 18.7190 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1440.4905 - mae: 19.7802 - val_loss: 979.6260 - val_mae: 18.7008 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1440.4905 - mae: 19.7802 - val_loss: 979.6260 - val_mae: 18.7008 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1459.5031 - mae: 19.3974 - val_loss: 925.9604 - val_mae: 18.1248 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1459.5031 - mae: 19.3974 - val_loss: 925.9604 - val_mae: 18.1248 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1427.7843 - mae: 19.3345 - val_loss: 881.4108 - val_mae: 17.4652 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1427.7843 - mae: 19.3345 - val_loss: 881.4108 - val_mae: 17.4652 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1444.0038 - mae: 19.4595 - val_loss: 902.5157 - val_mae: 17.9344 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1444.0038 - mae: 19.4595 - val_loss: 902.5157 - val_mae: 17.9344 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1424.6792 - mae: 19.0419 - val_loss: 846.8031 - val_mae: 16.8976 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1424.6792 - mae: 19.0419 - val_loss: 846.8031 - val_mae: 16.8976 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1413.4138 - mae: 18.9630 - val_loss: 844.6124 - val_mae: 17.4558 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1413.4138 - mae: 18.9630 - val_loss: 844.6124 - val_mae: 17.4558 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1370.7839 - mae: 18.8276 - val_loss: 838.2462 - val_mae: 17.3678 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1370.7839 - mae: 18.8276 - val_loss: 838.2462 - val_mae: 17.3678 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1369.2960 - mae: 18.8234 - val_loss: 800.6417 - val_mae: 17.1976 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1369.2960 - mae: 18.8234 - val_loss: 800.6417 - val_mae: 17.1976 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1462.6987 - mae: 20.6207 - val_loss: 830.8799 - val_mae: 18.0759 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1462.6987 - mae: 20.6207 - val_loss: 830.8799 - val_mae: 18.0759 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1400.3628 - mae: 19.2706 - val_loss: 775.1065 - val_mae: 16.8630 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1400.3628 - mae: 19.2706 - val_loss: 775.1065 - val_mae: 16.8630 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1394.4891 - mae: 19.3171 - val_loss: 836.6161 - val_mae: 18.2941 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1394.4891 - mae: 19.3171 - val_loss: 836.6161 - val_mae: 18.2941 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1405.1438 - mae: 19.3672 - val_loss: 790.9512 - val_mae: 17.4778 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1405.1438 - mae: 19.3672 - val_loss: 790.9512 - val_mae: 17.4778 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1435.9365 - mae: 19.8177 - val_loss: 774.0585 - val_mae: 17.3040 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1435.9365 - mae: 19.8177 - val_loss: 774.0585 - val_mae: 17.3040 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1392.7308 - mae: 19.3625 - val_loss: 876.9138 - val_mae: 20.2758 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1392.7308 - mae: 19.3625 - val_loss: 876.9138 - val_mae: 20.2758 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1508.1794 - mae: 20.8720 - val_loss: 817.9877 - val_mae: 19.1416 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1508.1794 - mae: 20.8720 - val_loss: 817.9877 - val_mae: 19.1416 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1389.6127 - mae: 19.1996 - val_loss: 740.9467 - val_mae: 16.9018 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1389.6127 - mae: 19.1996 - val_loss: 740.9467 - val_mae: 16.9018 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1277.3983 - mae: 18.3341 - val_loss: 724.9943 - val_mae: 16.8892 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1277.3983 - mae: 18.3341 - val_loss: 724.9943 - val_mae: 16.8892 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1346.3634 - mae: 19.7658 - val_loss: 1761.9055 - val_mae: 34.8988 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1346.3634 - mae: 19.7658 - val_loss: 1761.9055 - val_mae: 34.8988 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1447.6248 - mae: 19.6146 - val_loss: 726.6128 - val_mae: 17.0056 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1447.6248 - mae: 19.6146 - val_loss: 726.6128 - val_mae: 17.0056 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1268.3265 - mae: 18.1576 - val_loss: 710.5292 - val_mae: 16.6732 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1268.3265 - mae: 18.1576 - val_loss: 710.5292 - val_mae: 16.6732 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1332.7131 - mae: 18.3717 - val_loss: 702.9109 - val_mae: 16.1614 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1332.7131 - mae: 18.3717 - val_loss: 702.9109 - val_mae: 16.1614 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1239.9440 - mae: 17.9407 - val_loss: 696.5320 - val_mae: 16.0192 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1239.9440 - mae: 17.9407 - val_loss: 696.5320 - val_mae: 16.0192 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1364.7218 - mae: 18.3174 - val_loss: 704.9048 - val_mae: 16.5273 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1364.7218 - mae: 18.3174 - val_loss: 704.9048 - val_mae: 16.5273 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1338.8645 - mae: 18.0939 - val_loss: 710.0496 - val_mae: 16.7033 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1338.8645 - mae: 18.0939 - val_loss: 710.0496 - val_mae: 16.7033 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1284.8867 - mae: 18.2159 - val_loss: 692.0272 - val_mae: 16.1190 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1284.8867 - mae: 18.2159 - val_loss: 692.0272 - val_mae: 16.1190 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1270.3782 - mae: 17.8821 - val_loss: 675.6782 - val_mae: 16.1277 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1270.3782 - mae: 17.8821 - val_loss: 675.6782 - val_mae: 16.1277 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1339.7419 - mae: 18.2295 - val_loss: 711.0058 - val_mae: 17.0073 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1339.7419 - mae: 18.2295 - val_loss: 711.0058 - val_mae: 17.0073 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1301.0961 - mae: 18.2539 - val_loss: 696.9813 - val_mae: 16.8806 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1301.0961 - mae: 18.2539 - val_loss: 696.9813 - val_mae: 16.8806 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1291.4005 - mae: 18.1506 - val_loss: 678.0040 - val_mae: 16.4426 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1291.4005 - mae: 18.1506 - val_loss: 678.0040 - val_mae: 16.4426 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1400.4633 - mae: 18.3807 - val_loss: 668.2495 - val_mae: 15.9354 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1400.4633 - mae: 18.3807 - val_loss: 668.2495 - val_mae: 15.9354 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1232.2271 - mae: 17.8389 - val_loss: 692.7452 - val_mae: 16.4342 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1232.2271 - mae: 17.8389 - val_loss: 692.7452 - val_mae: 16.4342 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1234.1030 - mae: 17.9317 - val_loss: 684.9980 - val_mae: 16.8004 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1234.1030 - mae: 17.9317 - val_loss: 684.9980 - val_mae: 16.8004 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1371.8993 - mae: 18.1997 - val_loss: 666.1403 - val_mae: 15.8427 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1371.8993 - mae: 18.1997 - val_loss: 666.1403 - val_mae: 15.8427 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1259.3159 - mae: 18.1524 - val_loss: 664.4547 - val_mae: 16.1750 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1259.3159 - mae: 18.1524 - val_loss: 664.4547 - val_mae: 16.1750 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1258.8378 - mae: 17.8401 - val_loss: 653.8118 - val_mae: 15.9345 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1258.8378 - mae: 17.8401 - val_loss: 653.8118 - val_mae: 15.9345 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1295.4973 - mae: 18.2022 - val_loss: 680.8389 - val_mae: 17.0501 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1295.4973 - mae: 18.2022 - val_loss: 680.8389 - val_mae: 17.0501 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1242.9592 - mae: 17.8315 - val_loss: 657.1782 - val_mae: 16.2698 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1242.9592 - mae: 17.8315 - val_loss: 657.1782 - val_mae: 16.2698 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1285.8518 - mae: 18.2452 - val_loss: 666.4316 - val_mae: 16.3998 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1285.8518 - mae: 18.2452 - val_loss: 666.4316 - val_mae: 16.3998 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1320.7195 - mae: 18.1370 - val_loss: 664.0588 - val_mae: 16.3437 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1320.7195 - mae: 18.1370 - val_loss: 664.0588 - val_mae: 16.3437 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1259.2737 - mae: 18.1929 - val_loss: 666.3756 - val_mae: 15.8076 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1259.2737 - mae: 18.1929 - val_loss: 666.3756 - val_mae: 15.8076 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1193.0344 - mae: 17.9294 - val_loss: 663.1252 - val_mae: 16.1340 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1193.0344 - mae: 17.9294 - val_loss: 663.1252 - val_mae: 16.1340 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1278.8257 - mae: 17.9904 - val_loss: 677.8495 - val_mae: 17.0398 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1278.8257 - mae: 17.9904 - val_loss: 677.8495 - val_mae: 17.0398 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1254.3599 - mae: 17.8262 - val_loss: 659.2190 - val_mae: 15.9183 - learning_rate: 5.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1254.3599 - mae: 17.8262 - val_loss: 659.2190 - val_mae: 15.9183 - learning_rate: 5.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1271.5609 - mae: 17.7270 - val_loss: 634.4476 - val_mae: 15.3251 - learning_rate: 5.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1271.5609 - mae: 17.7270 - val_loss: 634.4476 - val_mae: 15.3251 - learning_rate: 5.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1218.8818 - mae: 17.6499 - val_loss: 666.6283 - val_mae: 16.1028 - learning_rate: 5.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1218.8818 - mae: 17.6499 - val_loss: 666.6283 - val_mae: 16.1028 - learning_rate: 5.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1192.4799 - mae: 17.5079 - val_loss: 645.3492 - val_mae: 15.5058 - learning_rate: 5.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1192.4799 - mae: 17.5079 - val_loss: 645.3492 - val_mae: 15.5058 - learning_rate: 5.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1281.7185 - mae: 17.9607 - val_loss: 668.9780 - val_mae: 16.6610 - learning_rate: 5.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1281.7185 - mae: 17.9607 - val_loss: 668.9780 - val_mae: 16.6610 - learning_rate: 5.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1327.5618 - mae: 18.1316 - val_loss: 670.9980 - val_mae: 16.3357 - learning_rate: 5.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1327.5618 - mae: 18.1316 - val_loss: 670.9980 - val_mae: 16.3357 - learning_rate: 5.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1256.4780 - mae: 17.8327 - val_loss: 643.4642 - val_mae: 15.5978 - learning_rate: 5.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1256.4780 - mae: 17.8327 - val_loss: 643.4642 - val_mae: 15.5978 - learning_rate: 5.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1209.8098 - mae: 17.7673 - val_loss: 661.6804 - val_mae: 16.4373 - learning_rate: 5.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1209.8098 - mae: 17.7673 - val_loss: 661.6804 - val_mae: 16.4373 - learning_rate: 5.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1187.5342 - mae: 17.6571 - val_loss: 680.3210 - val_mae: 16.3907 - learning_rate: 5.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1187.5342 - mae: 17.6571 - val_loss: 680.3210 - val_mae: 16.3907 - learning_rate: 5.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1243.0365 - mae: 17.6455 - val_loss: 664.1514 - val_mae: 16.1799 - learning_rate: 2.5000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1243.0365 - mae: 17.6455 - val_loss: 664.1514 - val_mae: 16.1799 - learning_rate: 2.5000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1297.7772 - mae: 17.9587 - val_loss: 650.4501 - val_mae: 15.8475 - learning_rate: 2.5000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1297.7772 - mae: 17.9587 - val_loss: 650.4501 - val_mae: 15.8475 - learning_rate: 2.5000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1204.6215 - mae: 17.4583 - val_loss: 643.1104 - val_mae: 15.8455 - learning_rate: 2.5000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1204.6215 - mae: 17.4583 - val_loss: 643.1104 - val_mae: 15.8455 - learning_rate: 2.5000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1286.1355 - mae: 17.7855 - val_loss: 655.8090 - val_mae: 16.1629 - learning_rate: 2.5000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1286.1355 - mae: 17.7855 - val_loss: 655.8090 - val_mae: 16.1629 - learning_rate: 2.5000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1283.3400 - mae: 17.7143 - val_loss: 661.2238 - val_mae: 16.1695 - learning_rate: 2.5000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1283.3400 - mae: 17.7143 - val_loss: 661.2238 - val_mae: 16.1695 - learning_rate: 2.5000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1275.5292 - mae: 17.7260 - val_loss: 650.3033 - val_mae: 15.8705 - learning_rate: 2.5000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1275.5292 - mae: 17.7260 - val_loss: 650.3033 - val_mae: 15.8705 - learning_rate: 2.5000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1258.4143 - mae: 17.7301 - val_loss: 639.6880 - val_mae: 15.3713 - learning_rate: 2.5000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1258.4143 - mae: 17.7301 - val_loss: 639.6880 - val_mae: 15.3713 - learning_rate: 2.5000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1280.3746 - mae: 17.7531 - val_loss: 646.9244 - val_mae: 15.6382 - learning_rate: 1.2500e-04\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1280.3746 - mae: 17.7531 - val_loss: 646.9244 - val_mae: 15.6382 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RNN: TRAINING COMPLETATO\n",
      "======================================================================\n",
      "✓ Modello salvato: rnn_pm25_forecaster_fixed_dropout.h5\n",
      "✓ History salvata: rnn_training_history_fixed_dropout.pkl\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training finale RNN con best params\n",
    "\n",
    "if rnn_model_path.exists():\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ RNN: Modello finale esistente - CARICAMENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rnn_model = keras.models.load_model(rnn_model_path)\n",
    "    \n",
    "    with open(rnn_history_path, 'rb') as f:\n",
    "        rnn_history = pickle.load(f)\n",
    "    \n",
    "    print(f\"Modello caricato da: {rnn_model_path.name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"RNN: TRAINING FINALE CON BEST PARAMS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rnn_model = create_rnn_model(\n",
    "        units=rnn_best_params['units'],\n",
    "        dropout=0.1,#rnn_best_params['dropout'],\n",
    "        learning_rate=rnn_best_params['learning_rate'],\n",
    "        n_layers=rnn_best_params['n_layers'],\n",
    "        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRNN Model Summary:\")\n",
    "    rnn_model.summary()\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INIZIO TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    history = rnn_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=rnn_best_params['batch_size'],\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    rnn_model.save(rnn_model_path)\n",
    "    \n",
    "    rnn_history = history.history\n",
    "    with open(rnn_history_path, 'wb') as f:\n",
    "        pickle.dump(rnn_history, f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RNN: TRAINING COMPLETATO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"✓ Modello salvato: {rnn_model_path.name}\")\n",
    "    print(f\"✓ History salvata: {rnn_history_path.name}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94866eae",
   "metadata": {},
   "source": [
    "# LSTM Model with Hyperparameter Optimization\n",
    "\n",
    "Implementazione di **LSTM** per completare il confronto con GRU e RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9be0e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths LSTM configurati:\n",
      "   LSTM model: lstm_pm25_forecaster_fixed_dropout.h5\n",
      "   LSTM config: lstm_best_params_fixed_dropout.pkl\n",
      "   LSTM history: lstm_training_history_fixed_dropout.pkl\n"
     ]
    }
   ],
   "source": [
    "# Configura paths per LSTM\n",
    "lstm_model_path = models_dir / 'lstm_pm25_forecaster_fixed_dropout.h5'\n",
    "lstm_config_path = models_dir / 'lstm_best_params_fixed_dropout.pkl'\n",
    "lstm_history_path = models_dir / 'lstm_training_history_fixed_dropout.pkl'\n",
    "\n",
    "print(f\"Paths LSTM configurati:\")\n",
    "print(f\"   LSTM model: {lstm_model_path.name}\")\n",
    "print(f\"   LSTM config: {lstm_config_path.name}\")\n",
    "print(f\"   LSTM history: {lstm_history_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c4cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Funzione create_lstm_model definita\n"
     ]
    }
   ],
   "source": [
    "def create_lstm_model(units, dropout, learning_rate, n_layers, input_shape):\n",
    "    \"\"\"\n",
    "    Crea modello LSTM per time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    units : int\n",
    "        Numero di unità LSTM\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    learning_rate : float\n",
    "        Learning rate per Adam optimizer\n",
    "    n_layers : int\n",
    "        Numero di layer LSTM da impilare\n",
    "    input_shape : tuple\n",
    "        (timesteps, features)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Modello LSTM compilato\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Primo layer\n",
    "    if n_layers == 1:\n",
    "        model.add(keras.layers.LSTM(units, dropout=dropout, input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(keras.layers.LSTM(units, dropout=dropout, return_sequences=True, input_shape=input_shape))\n",
    "        \n",
    "        # Layer intermedi\n",
    "        for i in range(1, n_layers - 1):\n",
    "            model.add(keras.layers.LSTM(units, dropout=dropout, return_sequences=True))\n",
    "        \n",
    "        # Ultimo layer\n",
    "        model.add(keras.layers.LSTM(units, dropout=dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Funzione create_lstm_model definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cc9939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function per Optuna - LSTM\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'units': trial.suggest_categorical('units', [32, 64, 128]),\n",
    "        'dropout': 0.1,  # Fisso a 0.1\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [1e-3, 5e-4]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128]),\n",
    "        'n_layers': trial.suggest_int('n_layers', 1, 3)  # 1-3 layer LSTM\n",
    "    }\n",
    "    \n",
    "    model = create_lstm_model(\n",
    "        units=params['units'],\n",
    "        dropout=params['dropout'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_layers=params['n_layers'],\n",
    "        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    )\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:19:00,477] A new study created in memory with name: lstm_pm25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LSTM: OTTIMIZZAZIONE HYPERPARAMETERS\n",
      "======================================================================\n",
      "   • Trials: 15\n",
      "   • Max epochs per trial: 50\n",
      "   • Early stopping: patience=5\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1063.74:   7%|▋         | 1/15 [02:06<29:24, 126.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:21:06,508] Trial 0 finished with value: 1063.7373046875 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 0 with value: 1063.7373046875.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1063.74:  13%|█▎        | 2/15 [08:50<1:02:46, 289.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:27:50,869] Trial 1 finished with value: 4398.2470703125 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 128, 'n_layers': 3}. Best is trial 0 with value: 1063.7373046875.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  20%|██        | 3/15 [32:58<2:43:41, 818.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:51:58,542] Trial 2 finished with value: 649.8850708007812 and parameters: {'units': 128, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 2}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  27%|██▋       | 4/15 [35:07<1:40:11, 546.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 10:54:08,079] Trial 3 finished with value: 1070.4688720703125 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  33%|███▎      | 5/15 [44:55<1:33:35, 561.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 11:03:56,253] Trial 4 finished with value: 3117.87548828125 and parameters: {'units': 64, 'learning_rate': 0.0005, 'batch_size': 128, 'n_layers': 2}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  40%|████      | 6/15 [47:04<1:02:10, 414.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 11:06:05,287] Trial 5 finished with value: 1058.45068359375 and parameters: {'units': 32, 'learning_rate': 0.001, 'batch_size': 64, 'n_layers': 1}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  47%|████▋     | 7/15 [51:22<48:24, 363.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 11:10:22,662] Trial 6 finished with value: 9578.9033203125 and parameters: {'units': 32, 'learning_rate': 0.0005, 'batch_size': 128, 'n_layers': 2}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  53%|█████▎    | 8/15 [53:02<32:36, 279.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 11:12:03,314] Trial 7 finished with value: 6933.87548828125 and parameters: {'units': 32, 'learning_rate': 0.0005, 'batch_size': 128, 'n_layers': 1}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  60%|██████    | 9/15 [1:17:16<1:04:40, 646.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 11:36:17,203] Trial 8 finished with value: 1541.5333251953125 and parameters: {'units': 128, 'learning_rate': 0.0005, 'batch_size': 128, 'n_layers': 2}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 649.885:  67%|██████▋   | 10/15 [1:42:22<1:16:00, 912.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 12:01:23,399] Trial 9 finished with value: 693.040771484375 and parameters: {'units': 128, 'learning_rate': 0.0005, 'batch_size': 64, 'n_layers': 2}. Best is trial 2 with value: 649.8850708007812.\n"
     ]
    }
   ],
   "source": [
    "# Ottimizzazione LSTM con Optuna (15 trials)\n",
    "\n",
    "if lstm_config_path.exists():\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ LSTM: Best params esistenti - CARICAMENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(lstm_config_path, 'rb') as f:\n",
    "        lstm_best_params = pickle.load(f)\n",
    "    \n",
    "    print(\"\\nLSTM Best hyperparameters:\")\n",
    "    for key, value in lstm_best_params.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"LSTM: OTTIMIZZAZIONE HYPERPARAMETERS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   • Trials: 15\")\n",
    "    print(f\"   • Max epochs per trial: 50\")\n",
    "    print(f\"   • Early stopping: patience=5\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name='lstm_pm25',\n",
    "        direction='minimize',\n",
    "        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        lstm_objective,\n",
    "        n_trials=15,\n",
    "        timeout=None,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LSTM: OTTIMIZZAZIONE COMPLETATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lstm_best_params = study.best_trial.params\n",
    "    best_val_loss = study.best_trial.value\n",
    "    \n",
    "    print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in lstm_best_params.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    with open(lstm_config_path, 'wb') as f:\n",
    "        pickle.dump(lstm_best_params, f)\n",
    "    \n",
    "    print(f\"\\n✓ Best params salvati: {lstm_config_path.name}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e450c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training finale LSTM con best params\n",
    "\n",
    "if lstm_model_path.exists():\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ LSTM: Modello finale esistente - CARICAMENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lstm_model = keras.models.load_model(lstm_model_path)\n",
    "    \n",
    "    with open(lstm_history_path, 'rb') as f:\n",
    "        lstm_history = pickle.load(f)\n",
    "    \n",
    "    print(f\"Modello caricato da: {lstm_model_path.name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"LSTM: TRAINING FINALE CON BEST PARAMS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lstm_model = create_lstm_model(\n",
    "        units=lstm_best_params['units'],\n",
    "        dropout=0.1,#lstm_best_params['dropout'],\n",
    "        learning_rate=lstm_best_params['learning_rate'],\n",
    "        n_layers=lstm_best_params['n_layers'],\n",
    "        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLSTM Model Summary:\")\n",
    "    lstm_model.summary()\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INIZIO TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=lstm_best_params['batch_size'],\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    lstm_model.save(lstm_model_path)\n",
    "    \n",
    "    lstm_history = history.history\n",
    "    with open(lstm_history_path, 'wb') as f:\n",
    "        pickle.dump(lstm_history, f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LSTM: TRAINING COMPLETATO\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"✓ Modello salvato: {lstm_model_path.name}\")\n",
    "    print(f\"✓ History salvata: {lstm_history_path.name}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a31670",
   "metadata": {},
   "source": [
    "# Model Comparison & Evaluation\n",
    "\n",
    "Confronto delle performance di **LSTM**, **GRU** e **RNN** sul test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione tutti i modelli sul test set\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALUTAZIONE SUL TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# GRU\n",
    "y_pred_gru = gru_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "gru_mse = mean_squared_error(y_test, y_pred_gru)\n",
    "gru_mae = mean_absolute_error(y_test, y_pred_gru)\n",
    "gru_r2 = r2_score(y_test, y_pred_gru)\n",
    "gru_rmse = np.sqrt(gru_mse)\n",
    "\n",
    "results['GRU'] = {\n",
    "    'predictions': y_pred_gru,\n",
    "    'MSE': gru_mse,\n",
    "    'RMSE': gru_rmse,\n",
    "    'MAE': gru_mae,\n",
    "    'R²': gru_r2\n",
    "}\n",
    "\n",
    "print(\"\\n🔷 GRU Performance:\")\n",
    "print(f\"   MSE:  {gru_mse:.4f}\")\n",
    "print(f\"   RMSE: {gru_rmse:.4f}\")\n",
    "print(f\"   MAE:  {gru_mae:.4f}\")\n",
    "print(f\"   R²:   {gru_r2:.4f}\")\n",
    "\n",
    "# RNN\n",
    "y_pred_rnn = rnn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "rnn_mse = mean_squared_error(y_test, y_pred_rnn)\n",
    "rnn_mae = mean_absolute_error(y_test, y_pred_rnn)\n",
    "rnn_r2 = r2_score(y_test, y_pred_rnn)\n",
    "rnn_rmse = np.sqrt(rnn_mse)\n",
    "\n",
    "results['RNN'] = {\n",
    "    'predictions': y_pred_rnn,\n",
    "    'MSE': rnn_mse,\n",
    "    'RMSE': rnn_rmse,\n",
    "    'MAE': rnn_mae,\n",
    "    'R²': rnn_r2\n",
    "}\n",
    "\n",
    "print(\"\\n🔶 RNN Performance:\")\n",
    "print(f\"   MSE:  {rnn_mse:.4f}\")\n",
    "print(f\"   RMSE: {rnn_rmse:.4f}\")\n",
    "print(f\"   MAE:  {rnn_mae:.4f}\")\n",
    "print(f\"   R²:   {rnn_r2:.4f}\")\n",
    "\n",
    "# LSTM\n",
    "y_pred_lstm = lstm_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "lstm_mse = mean_squared_error(y_test, y_pred_lstm)\n",
    "lstm_mae = mean_absolute_error(y_test, y_pred_lstm)\n",
    "lstm_r2 = r2_score(y_test, y_pred_lstm)\n",
    "lstm_rmse = np.sqrt(lstm_mse)\n",
    "\n",
    "results['LSTM'] = {\n",
    "    'predictions': y_pred_lstm,\n",
    "    'MSE': lstm_mse,\n",
    "    'RMSE': lstm_rmse,\n",
    "    'MAE': lstm_mae,\n",
    "    'R²': lstm_r2\n",
    "}\n",
    "\n",
    "print(\"\\n🔵 LSTM Performance:\")\n",
    "print(f\"   MSE:  {lstm_mse:.4f}\")\n",
    "print(f\"   RMSE: {lstm_rmse:.4f}\")\n",
    "print(f\"   MAE:  {lstm_mae:.4f}\")\n",
    "print(f\"   R²:   {lstm_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabella comparativa metriche\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABELLA COMPARATIVA METRICHE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    model: {\n",
    "        'MSE': data['MSE'],\n",
    "        'RMSE': data['RMSE'],\n",
    "        'MAE': data['MAE'],\n",
    "        'R²': data['R²']\n",
    "    }\n",
    "    for model, data in results.items()\n",
    "}).T\n",
    "\n",
    "metrics_df = metrics_df.round(4)\n",
    "print(metrics_df)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identifica miglior modello\n",
    "best_model = metrics_df['RMSE'].idxmin()\n",
    "print(f\"\\n🏆 MIGLIOR MODELLO (RMSE): {best_model}\")\n",
    "print(f\"   RMSE: {metrics_df.loc[best_model, 'RMSE']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5040099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione comparativa - Metrics Bar Plot\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'R²']\n",
    "colors = {'GRU': '#2E86AB', 'RNN': '#A23B72', 'LSTM': '#F18F01'}\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    models = list(results.keys())\n",
    "    values = [results[m][metric] for m in models]\n",
    "    \n",
    "    bars = ax.bar(models, values, color=[colors.get(m, '#999999') for m in models], alpha=0.8)\n",
    "    \n",
    "    # Evidenzia miglior modello\n",
    "    if metric != 'R²':\n",
    "        best_idx = np.argmin(values)\n",
    "    else:\n",
    "        best_idx = np.argmax(values)\n",
    "    bars[best_idx].set_edgecolor('gold')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Annota valori\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Salva figura\n",
    "fig_path = Path('../figures/prediction')\n",
    "fig_path.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_path / 'model_comparison_metrics.pdf', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Metrics comparison salvata: {fig_path / 'model_comparison_metrics.pdf'}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual (tutti i modelli)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))\n",
    "\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, data) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test, data['predictions'], alpha=0.5, s=10, \n",
    "               color=colors.get(model_name, '#999999'), label=model_name)\n",
    "    \n",
    "    # Linea ideale (y = x)\n",
    "    min_val = min(y_test.min(), data['predictions'].min())\n",
    "    max_val = max(y_test.max(), data['predictions'].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Ideale')\n",
    "    \n",
    "    ax.set_xlabel('Actual PM2.5 (μg/m³)', fontsize=11)\n",
    "    ax.set_ylabel('Predicted PM2.5 (μg/m³)', fontsize=11)\n",
    "    ax.set_title(f'{model_name}\\nR² = {data[\"R²\"]:.4f}, RMSE = {data[\"RMSE\"]:.4f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path / 'predicted_vs_actual_comparison.pdf', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Predicted vs Actual salvata: {fig_path / 'predicted_vs_actual_comparison.pdf'}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a95c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots (tutti i modelli)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))\n",
    "\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, data) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    residuals = y_test.flatten() - data['predictions']\n",
    "    \n",
    "    # Scatter plot residui\n",
    "    ax.scatter(data['predictions'], residuals, alpha=0.5, s=10,\n",
    "               color=colors.get(model_name, '#999999'))\n",
    "    \n",
    "    # Linea zero\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Predicted PM2.5 (μg/m³)', fontsize=11)\n",
    "    ax.set_ylabel('Residuals (μg/m³)', fontsize=11)\n",
    "    ax.set_title(f'{model_name} - Residual Plot\\nMAE = {data[\"MAE\"]:.4f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path / 'residual_plots_comparison.pdf', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Residual plots salvati: {fig_path / 'residual_plots_comparison.pdf'}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot: confronto predizioni su subset del test\n",
    "\n",
    "# Seleziona prime 500 predizioni per visualizzazione\n",
    "n_samples = min(500, len(y_test))\n",
    "sample_indices = range(n_samples)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Plot actual values\n",
    "ax.plot(sample_indices, y_test.flatten()[:n_samples], \n",
    "        label='Actual', color='black', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Plot predizioni di ogni modello\n",
    "for model_name, data in results.items():\n",
    "    ax.plot(sample_indices, data['predictions'][:n_samples],\n",
    "            label=f'{model_name} (RMSE={data[\"RMSE\"]:.2f})',\n",
    "            color=colors.get(model_name, '#999999'),\n",
    "            linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Sample Index', fontsize=12)\n",
    "ax.set_ylabel('PM2.5 (μg/m³)', fontsize=12)\n",
    "ax.set_title('Time Series Predictions Comparison (First 500 Samples)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path / 'time_series_comparison.pdf', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Time series comparison salvata: {fig_path / 'time_series_comparison.pdf'}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb45997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))\n",
    "\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, data) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    errors = np.abs(y_test.flatten() - data['predictions'])\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(errors, bins=50, color=colors.get(model_name, '#999999'), \n",
    "            alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Linea media\n",
    "    mean_error = np.mean(errors)\n",
    "    ax.axvline(mean_error, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean = {mean_error:.2f}')\n",
    "    \n",
    "    ax.set_xlabel('Absolute Error (μg/m³)', fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(f'{model_name} - Error Distribution\\nMAE = {data[\"MAE\"]:.4f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path / 'error_distribution_comparison.pdf', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Error distribution salvata: {fig_path / 'error_distribution_comparison.pdf'}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss comparison\n",
    "ax = axes[0]\n",
    "ax.plot(gru_history['loss'], label='GRU Train', color=colors['GRU'], linewidth=2)\n",
    "ax.plot(gru_history['val_loss'], label='GRU Val', color=colors['GRU'], \n",
    "        linewidth=2, linestyle='--')\n",
    "ax.plot(rnn_history['loss'], label='RNN Train', color=colors['RNN'], linewidth=2)\n",
    "ax.plot(rnn_history['val_loss'], label='RNN Val', color=colors['RNN'], \n",
    "        linewidth=2, linestyle='--')\n",
    "ax.plot(lstm_history['loss'], label='LSTM Train', color=colors['LSTM'], linewidth=2)\n",
    "ax.plot(lstm_history['val_loss'], label='LSTM Val', color=colors['LSTM'], \n",
    "        linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=11)\n",
    "ax.set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "ax = axes[1]\n",
    "ax.plot(gru_history['mae'], label='GRU Train', color=colors['GRU'], linewidth=2)\n",
    "ax.plot(gru_history['val_mae'], label='GRU Val', color=colors['GRU'], \n",
    "        linewidth=2, linestyle='--')\n",
    "ax.plot(rnn_history['mae'], label='RNN Train', color=colors['RNN'], linewidth=2)\n",
    "ax.plot(rnn_history['val_mae'], label='RNN Val', color=colors['RNN'], \n",
    "        linewidth=2, linestyle='--')\n",
    "ax.plot(lstm_history['mae'], label='LSTM Train', color=colors['LSTM'], linewidth=2)\n",
    "ax.plot(lstm_history['val_mae'], label='LSTM Val', color=colors['LSTM'], \n",
    "        linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('MAE', fontsize=11)\n",
    "ax.set_title('Training & Validation MAE', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path / 'training_history_comparison.pdf', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Training history salvata: {fig_path / 'training_history_comparison.pdf'}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c913cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary finale\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY FINALE - CONFRONTO MODELLI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📊 METRICHE TEST SET:\")\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "print(f\"\\n\\n🏆 MIGLIOR MODELLO: {best_model}\")\n",
    "print(f\"   • RMSE: {metrics_df.loc[best_model, 'RMSE']:.4f}\")\n",
    "print(f\"   • MAE:  {metrics_df.loc[best_model, 'MAE']:.4f}\")\n",
    "print(f\"   • R²:   {metrics_df.loc[best_model, 'R²']:.4f}\")\n",
    "\n",
    "print(\"\\n📈 RANKING (basato su RMSE):\")\n",
    "ranking = metrics_df.sort_values('RMSE')\n",
    "for i, (model, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"   {i}. {model:<8} - RMSE: {row['RMSE']:.4f}\")\n",
    "\n",
    "print(\"\\n💾 FILE SALVATI:\")\n",
    "print(f\"   • GRU model:     {gru_model_path.name}\")\n",
    "print(f\"   • RNN model:     {rnn_model_path.name}\")\n",
    "print(f\"   • Visualizzazioni in: {fig_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zfit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
